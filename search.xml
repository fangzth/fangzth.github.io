<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>专利撰写学习笔记</title>
      <link href="/2023/02/22/%E4%B8%93%E5%88%A9%E6%92%B0%E5%86%99%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/02/22/%E4%B8%93%E5%88%A9%E6%92%B0%E5%86%99%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="专利申请注意事项"><a href="#专利申请注意事项" class="headerlink" title="专利申请注意事项"></a>专利申请注意事项</h3><hr><p>嘿嘿嘿，今天写完了第一份专利，发给导师改了。这算是最近唯一获得成就感的地方了，记录一下，顺便把寒假在家学习到专利撰写需要注意的事项总结一下。明天我要继续为第二份、第三份专利而努力了，毕竟导师的饼可是5<del>6个呢。 哈哈</del>加油！</p><h4 id="1-如何完成一份专利"><a href="#1-如何完成一份专利" class="headerlink" title="1. 如何完成一份专利"></a>1. 如何完成一份专利</h4><ul><li>专利类型：（1）外形、色彩等——外观设计专利；（2）结构方面——根据创新型的不同可选择实用新型专利和发明专利；（3）方法方面——属于发明专利</li><li>撰写方式：<ul><li>自己撰写：自行提交国家知识产权局（需要准备材料：说明书摘要、摘要附图、权力要求书、说明书），等待后期答复</li><li>知识产权代理公司：仅需准备交底书，由代理公司人员撰写成所需格式</li></ul></li></ul><h4 id="2-发明专利与实用新型专利的区别"><a href="#2-发明专利与实用新型专利的区别" class="headerlink" title="2. 发明专利与实用新型专利的区别"></a>2. 发明专利与实用新型专利的区别</h4><ul><li>在进步方面的不同<ul><li>发明专利需要具备“突出的实质性特点和显著的进步“</li><li>实用新型专利只需要具备”实质性特点和进步“</li></ul></li><li>审批流程不同<ul><li>对实用新型只进行初步审查</li><li>对发明专利除初步审查之外，还需要进行实质审查</li></ul></li><li>有效期不同<ul><li>发明专利有效期是20年</li><li>实用新型专利和外观设计专利都是10年</li></ul></li></ul><h4 id="3-需要准备材料"><a href="#3-需要准备材料" class="headerlink" title="3. 需要准备材料"></a>3. 需要准备材料</h4><ul><li>个人身份证或公司营业执照等资格证明材料</li><li>技术交底书（包括名称、技术领域、技术背景、发明内容、附图说明、具体实施方式等），[发明专利可不需要附图]</li></ul><h4 id="4-技术交底书写作注意事项"><a href="#4-技术交底书写作注意事项" class="headerlink" title="4. 技术交底书写作注意事项"></a>4. 技术交底书写作注意事项</h4><p>（1） 论文发表影响专利，专利不影响论文。专利申请之后可以投稿论文（<strong>专利申请日在论文发表之前不受影响</strong>）</p><p>（2）内容清楚、完整：要使代理人能够看懂，尤其是背景技术和详细技术方案，全面清楚</p><p>（3）一致性：对同一事物的叫法应统一，避免出现多种叫法，同一不见再不同附图中采用同一标号</p><p>（4）技术手段：技术方案不能只有原理（设想），不能只做功能性介绍，要有技术手段</p><p>（5）公开充分：专利必须充分公开，以本领域技术人员可实现为准，对于技术秘密，可做细节上处理</p><p>（6）英文缩写：英文缩写应有中文译文及英文全称。</p><h4 id="5-技术交底书内容"><a href="#5-技术交底书内容" class="headerlink" title="5. 技术交底书内容"></a>5. 技术交底书内容</h4><p>（2）技术领域部分：是要求保护的发明或实用新型技术方案所属或者直接应用的具体技术领域，而不是上位的或者是相邻的技术领域，也不是专利本身</p><p>（3）技术背景</p><ul><li>展示充分性：本发明的技术内容相关，并给出出处，写明对发明的理解、检索、审查有用的背景技术，有可能的，引证这些背景技术的文件</li><li>客观性：客观介绍现有技术的不足，本发明要解决的技术问题则水到渠成的引出</li></ul><p>（4）发明内容</p><ul><li>解决的技术问题、</li><li>清楚说明方案实现过程，表达方式不限：对于方法，应当说明方法的主要思路和具体步骤等</li><li>写明保护的技术点，技术保留处应明确交代</li><li>写明技术点是否有可替代方案</li><li>技术效果（有益效果）</li></ul><p>（5）说明书附图：是说明书的组成部分，必要时用来结束权利要求书。</p><ul><li>要求：不带颜色、线框图、清楚简单，一次到位，补图影响申请日期</li><li>例如：图1是本发明xxx的示意图；图2是xxx的立体图</li></ul><h4 id="6-代理机构撰写流程"><a href="#6-代理机构撰写流程" class="headerlink" title="6. 代理机构撰写流程"></a>6. 代理机构撰写流程</h4><p><strong>阅读交底→检索→沟通→撰写→发明人审核→递交</strong></p>]]></content>
      
      
      <categories>
          
          <category> 科研总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 经验总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2022年终总结</title>
      <link href="/2022/12/31/2022%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
      <url>/2022/12/31/2022%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h3 id="2022年总结"><a href="#2022年总结" class="headerlink" title="2022年总结"></a>2022年总结</h3><hr><p>2023年已经过去10天了。想来回家也有5天了，是时候补上去年该写的年终总结了。</p><p>这一年以来，经历 了有生以来最为魔幻的一年。前前后后经历了两次完全封校，国内其他地方更是在疫情防控中出现了一些离谱的事件，防疫政策也千变万化，可以说2022年完全是在疫情笼罩下艰难度过的一年。此外，国内外局势先瞬息万变，战争离我们并不遥远。这些让我对我们所生活的这个世界既痛恨又珍惜。</p><p>之前在个人评价里面对这一年以来的工作做了一个总结。就按部就班生活着，主要完成了导师交代的五院实习工作、一院的主动温控项目，并顺利通过了博资考。其于时间就在完成课题组内的一些杂活，好像也没干什么让自己记忆犹新的事。博士一年的生涯就这样过去了，尤其在课题和学术方面基本没有任何进展，这着实让人心焦。</p><p>再来说说好的一方面吧，这一年也并不是在摆烂中度过。除了完成既定的科研任务外，抽出了许多业余时间对自己进行了补充。例如，学习了CS106b、深度学习等一些课程，坚持体育锻炼，提高了健康意识等。经历了两位博士师兄的毕业，学到了很多，也感受到了博士毕业的压力。这一年的锻炼也使我成长了许多，在课题组内需要承担的责任越来越大，与导师交流时的心态也发生了变化，学会更多的去独立思考一些事情。但是也存在许多待改进的地方：</p><ul><li>想做的事，就立即去做，不能浪费时间在想做上，做行动上的巨人</li><li>要以主人公的姿态看待发生在自己身上的事，不能畏首畏尾</li><li>提高对团队的领导和捏合能力，现在课题组氛围营造得并不好</li><li>遇到问题不能先想到的是抱怨、追责，要先解决问题</li><li>得要开始学习待人接物，以前总是嗤之以鼻，但这才是工作中安身立命之本</li></ul><p>随着年龄的增大，一些之前觉得很遥远的事也逐渐逼近。老婆今年也顺利毕业，在凯里找了一份工作。和她的感情一如既往的好，今年吵架生气的次数也很少，只是买车、订婚、结婚这些事也得开始考虑了。</p><p>今日事，今日毕。现在，终于可以给这魔幻的2022年画上了句号。</p><p>2023年需要改变自己这个破拖延症，到现在要写的专利也还没有眉目，接下来几天得集中把这事完结了。</p><hr><h4 id="2023年目标"><a href="#2023年目标" class="headerlink" title="2023年目标"></a>2023年目标</h4><ul><li><p><input disabled="" type="checkbox"> 完成两项专利的申请</p></li><li><p><input disabled="" type="checkbox"> 发表1篇SCI</p></li><li><p><input disabled="" type="checkbox"> 通过雅思考试</p></li><li><p><input disabled="" type="checkbox"> 申请国外交流项目</p></li><li><p><input disabled="" type="checkbox"> 参加半马</p></li><li><p><input disabled="" type="checkbox"> 顺利通过开题</p></li><li><p><input disabled="" type="checkbox"> 订婚❤</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 科研总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 心得体会 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A05_Using convolutions to generalize</title>
      <link href="/2022/12/26/A05-Using-convolutions-to-generalize/"/>
      <url>/2022/12/26/A05-Using-convolutions-to-generalize/</url>
      
        <content type="html"><![CDATA[<h2 id="Using-convolutions-to-generalize"><a href="#Using-convolutions-to-generalize" class="headerlink" title="Using convolutions to generalize"></a>Using convolutions to generalize</h2><h3 id="1-Convolution"><a href="#1-Convolution" class="headerlink" title="1 Convolution"></a>1 Convolution</h3>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A04 Learning from images</title>
      <link href="/2022/11/26/A04-Learning-from-images/"/>
      <url>/2022/11/26/A04-Learning-from-images/</url>
      
        <content type="html"><![CDATA[<h2 id="Telling-birds-from-airplanes"><a href="#Telling-birds-from-airplanes" class="headerlink" title="Telling birds from airplanes"></a>Telling birds from airplanes</h2><h3 id="1-Basic-Dataset"><a href="#1-Basic-Dataset" class="headerlink" title="1. Basic Dataset"></a>1. Basic Dataset</h3><ul><li><p>MNIST：手写体图像识别数据集</p></li><li><p>CIFAR-10：由60000张32×32照片组成（RGB），从1-10分类.可使用torchvision库进行下载</p></li><li><p>torch.utils.data.Dataset库的两种方法：<strong><strong>len____和____getitem</strong></strong></p></li><li><p>transform:</p></li><li><p>Normalize：将每个通道归一化，使其有相同的分布（平均值为0，标准差相同），这样使其能够在同一学习率下进行梯度下降更新。其实现功能为：v_n[c] &#x3D; (v[c] - mean[c]) &#x2F; stdev[c]</p></li></ul><h3 id="Softmax分类器"><a href="#Softmax分类器" class="headerlink" title="Softmax分类器"></a>Softmax分类器</h3><ul><li>以概率的形式输出，范围在[0，1]之间； 所有元素概率之和为1</li><li>实质上：对所输入tensor做指数运算。单调函数</li><li>可以通过nn.Softmax来调用，但需要指定sofmax函数作用的维度</li></ul><h3 id="分类问题的损失函数"><a href="#分类问题的损失函数" class="headerlink" title="分类问题的损失函数"></a>分类问题的损失函数</h3><ul><li>NLL：负对数概率，NLL &#x3D; -sum(log(out_i[c_i]))，NLL以概率作为输入当概率增大时，NLL的值减小</li><li>PyTorch中通过nn.NLLLoss类来实现上述损失的计算，以概率的log值作为输入，然后计算batch的NLL。</li><li>为了维持计算的数值稳定性，其分类器应采用nn.LogSoftmax()</li><li>使用交叉熵损失来替代MSE。MSE的斜度变化不均匀，有的地方斜度大，有的斜度小，因此在接近目标时斜度较小；而交叉熵损失整个区间内都斜度比较平缓。</li><li>调用nn.CrossEntropyLoss可代替nn.Softmax和nn.NLLLoss起到的作用</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">torch.set_printoptions(edgeitems=<span class="number">2</span>, linewidth=<span class="number">75</span>)</span><br><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">class_names = [<span class="string">&#x27;airplane&#x27;</span>,<span class="string">&#x27;automobile&#x27;</span>,<span class="string">&#x27;bird&#x27;</span>,<span class="string">&#x27;cat&#x27;</span>,<span class="string">&#x27;deer&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;dog&#x27;</span>,<span class="string">&#x27;frog&#x27;</span>,<span class="string">&#x27;horse&#x27;</span>,<span class="string">&#x27;ship&#x27;</span>,<span class="string">&#x27;truck&#x27;</span>]</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line">data_path = <span class="string">&#x27;../data-unversioned/p1ch7/&#x27;</span></span><br><span class="line">cifar10 = datasets.CIFAR10(</span><br><span class="line">    data_path, train=<span class="literal">True</span>, download=<span class="literal">False</span>,</span><br><span class="line">    transform=transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">0.4915</span>, <span class="number">0.4823</span>, <span class="number">0.4468</span>),</span><br><span class="line">                             (<span class="number">0.2470</span>, <span class="number">0.2435</span>, <span class="number">0.2616</span>))</span><br><span class="line">    ]))</span><br><span class="line">cifar10_val = datasets.CIFAR10(</span><br><span class="line">    data_path, train=<span class="literal">False</span>, download=<span class="literal">False</span>,</span><br><span class="line">    transform=transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">0.4915</span>, <span class="number">0.4823</span>, <span class="number">0.4468</span>),</span><br><span class="line">                             (<span class="number">0.2470</span>, <span class="number">0.2435</span>, <span class="number">0.2616</span>))</span><br><span class="line">    ]))</span><br><span class="line">label_map = &#123;<span class="number">0</span>: <span class="number">0</span>, <span class="number">2</span>: <span class="number">1</span>&#125;</span><br><span class="line">class_names = [<span class="string">&#x27;airplane&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>]</span><br><span class="line">cifar2 = [(img, label_map[label])</span><br><span class="line">          <span class="keyword">for</span> img, label <span class="keyword">in</span> cifar10 </span><br><span class="line">          <span class="keyword">if</span> label <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">2</span>]]</span><br><span class="line">cifar2_val = [(img, label_map[label])</span><br><span class="line">              <span class="keyword">for</span> img, label <span class="keyword">in</span> cifar10_val</span><br><span class="line">              <span class="keyword">if</span> label <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">2</span>]]</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(cifar2, batch_size=<span class="number">64</span>,</span><br><span class="line">                                           shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">8</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">8</span> * <span class="number">8</span> * <span class="number">8</span>, <span class="number">32</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">32</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = F.max_pool2d(torch.relu(self.conv1(x)), <span class="number">2</span>)</span><br><span class="line">        out = F.max_pool2d(torch.relu(self.conv2(out)), <span class="number">2</span>)</span><br><span class="line">        out = out.view(-<span class="number">1</span>, <span class="number">8</span> * <span class="number">8</span> * <span class="number">8</span>)</span><br><span class="line">        out = torch.tanh(self.fc1(out))</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line">model = Net()</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">n_epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    <span class="keyword">for</span> imgs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        outputs = model(imgs)</span><br><span class="line">        loss = loss_fn(outputs, labels)</span><br><span class="line">                </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch: %d, Loss: %f&quot;</span> % (epoch, <span class="built_in">float</span>(loss)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Datasets and DataLoaders </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A03_Using a Network to fit the data</title>
      <link href="/2022/11/24/A03-Using-a-Network-to-fit-the-data/"/>
      <url>/2022/11/24/A03-Using-a-Network-to-fit-the-data/</url>
      
        <content type="html"><![CDATA[<h2 id="Using-a-Network-to-fit-the-data"><a href="#Using-a-Network-to-fit-the-data" class="headerlink" title="Using a Network to fit the data"></a>Using a Network to fit the data</h2><h3 id="1-Artificial-neurons"><a href="#1-Artificial-neurons" class="headerlink" title="1. Artificial neurons"></a>1. Artificial neurons</h3><ul><li>神经元：运用简单的函数组合来代表复杂函数的数学关系</li><li>激活函数：torch.nn.Sigmoid，包括1&#x2F;（1+e** -x)，tanh等将输出值压缩至[-1,1]或[0,1]</li><li>光滑的激活函数：Tanh，Softplus；不光滑函数：Hardtanh，ReLU（表现最好，被广泛使用）</li><li>Sigmoid函数：在早期神经网络中使用，能将输出投影到[0, 1]之间，例如概率</li><li>最佳激活函数的选择：<ul><li>特点：非线性、可微分</li><li>神经网络训练对激活函数的要求：存在敏感变化区、存在非敏感区</li><li>当输入趋于正无穷时输出上限，当输入趋于负无穷时输出下限</li></ul></li></ul><h3 id="2-The-PyTorch-nn-module"><a href="#2-The-PyTorch-nn-module" class="headerlink" title="2. The PyTorch nn module"></a>2. The PyTorch nn module</h3><ul><li>torch.nn 包含了构建人工神经网络的所有模块，模块包含很多子模块，如：nn.Module，nn.Linear等</li><li>批量输入：batch，1. 为了充分利用计算资源；2. 一些模型也需要利用批量信息</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line">t_c = [<span class="number">0.5</span>,  <span class="number">14.0</span>, <span class="number">15.0</span>, <span class="number">28.0</span>, <span class="number">11.0</span>,  <span class="number">8.0</span>,  <span class="number">3.0</span>, -<span class="number">4.0</span>,  <span class="number">6.0</span>, <span class="number">13.0</span>, <span class="number">21.0</span>]</span><br><span class="line">t_u = [<span class="number">35.7</span>, <span class="number">55.9</span>, <span class="number">58.2</span>, <span class="number">81.9</span>, <span class="number">56.3</span>, <span class="number">48.9</span>, <span class="number">33.9</span>, <span class="number">21.8</span>, <span class="number">48.4</span>, <span class="number">60.4</span>, <span class="number">68.4</span>]</span><br><span class="line">t_c = torch.tensor(t_c).unsqueeze(<span class="number">1</span>) <span class="comment"># &lt;1&gt;</span></span><br><span class="line">t_u = torch.tensor(t_u).unsqueeze(<span class="number">1</span>) <span class="comment"># &lt;1&gt;</span></span><br><span class="line">t_u.shape</span><br><span class="line">n_samples = t_u.shape[<span class="number">0</span>]</span><br><span class="line">n_val = <span class="built_in">int</span>(<span class="number">0.2</span> * n_samples)</span><br><span class="line"></span><br><span class="line">shuffled_indices = torch.randperm(n_samples)</span><br><span class="line"></span><br><span class="line">train_indices = shuffled_indices[:-n_val]</span><br><span class="line">val_indices = shuffled_indices[-n_val:]</span><br><span class="line"></span><br><span class="line">train_indices, val_indices</span><br><span class="line">t_u_train = t_u[train_indices]</span><br><span class="line">t_c_train = t_c[train_indices]</span><br><span class="line"></span><br><span class="line">t_u_val = t_u[val_indices]</span><br><span class="line">t_c_val = t_c[val_indices]</span><br><span class="line"></span><br><span class="line">t_un_train = <span class="number">0.1</span> * t_u_train</span><br><span class="line">t_un_val = <span class="number">0.1</span> * t_u_val</span><br><span class="line">linear_model = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">optimizer = optim.SGD(linear_model.parameters(), lr = <span class="number">1e-2</span>)</span><br><span class="line">linear_model.parameters()</span><br><span class="line"><span class="built_in">list</span>(linear_model.parameters())</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_loop</span>(<span class="params">n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs+<span class="number">1</span>):</span><br><span class="line">        t_p_train = model(t_u_train)</span><br><span class="line">        loss_train = loss_fn(t_p_train, t_c_train)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            t_p_val = model(t_u_val)</span><br><span class="line">            loss_val = loss_fn(t_p_val, t_c_val)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss_train.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">1</span> <span class="keyword">or</span> epoch % <span class="number">1000</span> ==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, Training loss <span class="subst">&#123;loss_train.item():<span class="number">.4</span>f&#125;</span>,&quot;</span></span><br><span class="line">                 <span class="string">f&quot;Validation loss <span class="subst">&#123;loss_val.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">linear_model = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">optimizer = optim.SGD(linear_model.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">training_loop(n_epochs=<span class="number">3000</span>, optimizer=optimizer, model=linear_model, loss_fn=nn.MSELoss(),</span><br><span class="line">              t_u_train=t_un_train, t_u_val=t_un_val, t_c_train=t_c_train, t_c_val=t_c_val)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(linear_model.weight)</span><br><span class="line"><span class="built_in">print</span>(linear_model.bias)</span><br><span class="line">seq_model = nn.Sequential(nn.Linear(<span class="number">1</span>, <span class="number">13</span>), nn.Tanh(), nn.Linear(<span class="number">13</span>, <span class="number">1</span>))</span><br><span class="line">seq_model</span><br><span class="line">[param.shape <span class="keyword">for</span> param <span class="keyword">in</span> seq_model.parameters()]</span><br><span class="line"><span class="comment"># 模型太对无法辨认出各模型的参数时，可使用named_parameters()函数</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> seq_model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name, param.shape)</span><br><span class="line"><span class="comment"># Sequential()中每个模块与其出现的序号相对应，意思就是把序号变成字符名称，便于解释阅读</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">seq_model = nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">&#x27;hidden_linear&#x27;</span>, nn.Linear(<span class="number">1</span>, <span class="number">8</span>)),</span><br><span class="line">    (<span class="string">&#x27;hidden_activation&#x27;</span>, nn.Tanh()),</span><br><span class="line">    (<span class="string">&#x27;output_linear&#x27;</span>, nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">]))</span><br><span class="line">seq_model</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(seq_model.parameters(), lr = <span class="number">1e-3</span>)</span><br><span class="line">training_loop(n_epochs=<span class="number">5000</span>, optimizer=optimizer, model=seq_model, loss_fn=nn.MSELoss(),</span><br><span class="line">              t_u_train=t_un_train, t_u_val=t_un_val, t_c_train=t_c_train, t_c_val=t_c_val)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output&#x27;</span>, seq_model(t_un_val))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;answer&#x27;</span>, t_c_val)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;hidden&#x27;</span>, seq_model.hidden_linear.weight.grad)</span><br><span class="line"><span class="comment">#可视化结果</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">t_range = torch.arange(<span class="number">20.</span>, <span class="number">90.</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">fig = plt.figure(dpi=<span class="number">600</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Fahrenheit&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Celsius&quot;</span>)</span><br><span class="line">plt.plot(t_u.numpy(), t_c.numpy(), <span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.plot(t_range.numpy(), seq_model(<span class="number">0.1</span> * t_range).detach().numpy(), <span class="string">&#x27;c-&#x27;</span>)</span><br><span class="line">plt.plot(t_u.numpy(), seq_model(<span class="number">0.1</span> * t_u).detach().numpy(), <span class="string">&#x27;kx&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nn.Module </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A02_The mechanics of learning</title>
      <link href="/2022/11/20/A02-The-mechanics-of-learning/"/>
      <url>/2022/11/20/A02-The-mechanics-of-learning/</url>
      
        <content type="html"><![CDATA[<h2 id="The-mechanics-of-learning"><a href="#The-mechanics-of-learning" class="headerlink" title="The mechanics of learning"></a>The mechanics of learning</h2><h3 id="1-参数计算"><a href="#1-参数计算" class="headerlink" title="1. 参数计算"></a>1. 参数计算</h3><ul><li>tensor计算具有一个特点：<strong>Broadcasting</strong>—进行计算时维度从后向前计数</li><li>torch.stack()表示把两个序列组合成新的tensor，torch.cat()也是组合tensor函数</li><li>*param表示按照其维度传递参数</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">t_c = [<span class="number">0.5</span>, <span class="number">14.0</span>, <span class="number">15.0</span>, <span class="number">28.0</span>, <span class="number">11.0</span>, <span class="number">8.0</span>, <span class="number">3.0</span>, -<span class="number">4.0</span>, <span class="number">6.0</span>, <span class="number">13.0</span>, <span class="number">21.0</span>]</span><br><span class="line">t_u = [<span class="number">35.7</span>, <span class="number">55.9</span>, <span class="number">58.2</span>, <span class="number">81.9</span>, <span class="number">56.3</span>, <span class="number">48.9</span>, <span class="number">33.9</span>, <span class="number">21.8</span>, <span class="number">48.4</span>, <span class="number">60.4</span>, <span class="number">68.4</span>]</span><br><span class="line">t_c = torch.tensor(t_c)</span><br><span class="line">t_u = torch.tensor(t_u)</span><br><span class="line">plt.plot(t_u, t_c, <span class="string">&#x27;ro&#x27;</span>)   <span class="comment"># &#x27;ro&#x27;表示散点图的意思</span></span><br><span class="line"><span class="comment"># 定义模型（线性）和损失</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">t_u, w, b</span>):</span><br><span class="line">    <span class="keyword">return</span> w * t_u +b</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_fn</span>(<span class="params">t_p, t_c</span>):</span><br><span class="line">    squared_diffs = (t_p - t_c)**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> squared_diffs.mean()</span><br><span class="line">w = torch.ones(())</span><br><span class="line">b = torch.zeros(())</span><br><span class="line">t_p = model(t_u, w, b)</span><br><span class="line">loss = loss_fn(t_p, t_c)</span><br><span class="line"><span class="comment"># Broadcasting</span></span><br><span class="line">x = torch.ones(())</span><br><span class="line">y = torch.ones(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">z = torch.ones(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">a = torch.ones(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;shapes: x: <span class="subst">&#123;x.shape&#125;</span>, y: <span class="subst">&#123;y.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot; z: <span class="subst">&#123;z.shape&#125;</span>, a: <span class="subst">&#123;a.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x * y:&quot;</span>, (x * y).shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y * z:&quot;</span>, (y * z).shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y * z * a:&quot;</span>, (y * z * a).shape)</span><br><span class="line"></span><br><span class="line">delta = <span class="number">0.1</span></span><br><span class="line">loss_rate_of_change_w = \</span><br><span class="line">    (loss_fn(model(t_u, w + delta, b), t_c) - loss_fn(model(t_u, w - delta, b), t_c)) / (<span class="number">2.0</span> * delta)</span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">w = w - learning_rate * loss_rate_of_change_w</span><br><span class="line">loss_rate_of_change_b = \</span><br><span class="line">    (loss_fn(model(t_u, w, b + delta), t_c) - loss_fn(model(t_u, w, b - delta), t_c)) / (<span class="number">2.0</span> * delta)</span><br><span class="line">b = b - learning_rate * loss_rate_of_change_b</span><br><span class="line"><span class="comment"># 这种计算loss值的方法存在弊端：当变化较快，delta捕捉不到时存在问题，因此需要计算梯度</span></span><br><span class="line"><span class="comment"># 计算参数的导数：链式法则，d loss_fn / d w = (d loss_fn / d t_p) * (d t_p / d w)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dloss_fn</span>(<span class="params">t_p, t_c</span>):</span><br><span class="line">    dsq_diffs = <span class="number">2</span> * (t_p - t_c) / t_p.size(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> dsq_diffs</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dmodel_dw</span>(<span class="params">t_u, w, b</span>):</span><br><span class="line">    <span class="keyword">return</span> t_u</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dmodel_db</span>(<span class="params">t_u, w, b</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">grad_fn</span>(<span class="params">t_u, t_c, t_p, w, b</span>):</span><br><span class="line">    dloss_dtp = dloss_fn(t_p, t_c)</span><br><span class="line">    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)</span><br><span class="line">    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)</span><br><span class="line">    <span class="keyword">return</span> torch.stack([dloss_dw.<span class="built_in">sum</span>(), dloss_db.<span class="built_in">sum</span>()])</span><br><span class="line"><span class="comment"># torch.stack()表示把两个序列组合成新的tensor,如两个3×3的，组成2×3×3的</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_loop</span>(<span class="params">n_epochs, learning_rate, params, t_u, t_c</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs +<span class="number">1</span>):</span><br><span class="line">        w, b = params</span><br><span class="line">        t_p = model(t_u, w, b)</span><br><span class="line">        loss = loss_fn(t_p, t_c)</span><br><span class="line">        grad = grad_fn(t_u, t_c, t_p, w, b)</span><br><span class="line">        params = params -learning_rate * grad</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch %d, Loss %f &#x27;</span> % (epoch, <span class="built_in">float</span>(loss)))</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line">training_loop(n_epochs=<span class="number">100</span>, learning_rate=<span class="number">1e-2</span>, params = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>]), t_u = t_u, t_c = t_c)</span><br><span class="line"><span class="comment"># 改变学习率</span></span><br><span class="line">training_loop(n_epochs=<span class="number">100</span>, learning_rate=<span class="number">1e-4</span>, params = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>]), t_u = t_u, t_c = t_c)</span><br><span class="line"><span class="comment"># 改变输入值的大小</span></span><br><span class="line">training_loop(n_epochs=<span class="number">100</span>, learning_rate=<span class="number">1e-2</span>, params = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>]), t_u = <span class="number">0.1</span> * t_u, t_c = t_c)</span><br><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line">params = training_loop(n_epochs = <span class="number">5000</span>,learning_rate = <span class="number">1e-2</span>,params = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>]),t_u = <span class="number">0.1</span> * t_u,t_c = t_c)</span><br><span class="line"><span class="comment"># argument unpacking: model(t_un, *params) is equivalent to model(t_un, params[0], params[1])</span></span><br><span class="line">t_p = model(t_un, *params)</span><br><span class="line">fig = plt.figure(dpi=<span class="number">600</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Temperature (°Fahrenheit)&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Temperature (°Celsius)&quot;</span>)</span><br><span class="line">plt.plot(t_u.numpy(), t_p.detach().numpy())</span><br><span class="line">plt.plot(t_u.numpy(), t_c.numpy(), <span class="string">&#x27;o&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="2-自动梯度计算"><a href="#2-自动梯度计算" class="headerlink" title="2. 自动梯度计算"></a>2. 自动梯度计算</h3><ul><li>pytorch能够自动构建计算图、自动计算梯度，需要在tensor生成时确定requires_grad&#x3D;True</li><li>注意进行梯度归零和不进行梯度计算申明</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">t_c = [<span class="number">0.5</span>, <span class="number">14.0</span>, <span class="number">15.0</span>, <span class="number">28.0</span>, <span class="number">11.0</span>, <span class="number">8.0</span>, <span class="number">3.0</span>, -<span class="number">4.0</span>, <span class="number">6.0</span>, <span class="number">13.0</span>, <span class="number">21.0</span>]</span><br><span class="line">t_u = [<span class="number">35.7</span>, <span class="number">55.9</span>, <span class="number">58.2</span>, <span class="number">81.9</span>, <span class="number">56.3</span>, <span class="number">48.9</span>, <span class="number">33.9</span>, <span class="number">21.8</span>, <span class="number">48.4</span>, <span class="number">60.4</span>, <span class="number">68.4</span>]</span><br><span class="line">t_c = torch.tensor(t_c)</span><br><span class="line">t_u = torch.tensor(t_u)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">t_u, w, b</span>):</span><br><span class="line">    <span class="keyword">return</span> w * t_u +b</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_fn</span>(<span class="params">t_p, t_c</span>):</span><br><span class="line">    squared_diffs = (t_p - t_c)**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> squared_diffs.mean()</span><br><span class="line">params = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>], requires_grad=<span class="literal">True</span>)   <span class="comment">#确定对该tensor计算梯度</span></span><br><span class="line">params.grad <span class="keyword">is</span> <span class="literal">None</span><span class="comment">#没有backward时，grad为空</span></span><br><span class="line">loss = loss_fn(model(t_u, *params), t_c)  <span class="comment"># *params是python中的unpacking表示，会按维度一一传入参数</span></span><br><span class="line">loss.backward()</span><br><span class="line">params.grad</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_loop</span>(<span class="params">n_epochs, learning_rate, params, t_u, t_c</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> params.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            params.grad.zero_()   <span class="comment">#梯度归零</span></span><br><span class="line">        t_p = model(t_u, *params)</span><br><span class="line">        loss = loss_fn(t_p, t_c)</span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 不计算梯度</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            params -= learning_rate * params.grad</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Epoch %d, Loss %f&#x27;</span> % (epoch, <span class="built_in">float</span>(loss)))</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line">training_loop(n_epochs=<span class="number">5000</span>, learning_rate=<span class="number">1e-2</span>, params= torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>], requires_grad=<span class="literal">True</span>), t_u=<span class="number">0.1</span> * t_u, t_c=t_c)</span><br></pre></td></tr></table></figure><h3 id="3-优化器"><a href="#3-优化器" class="headerlink" title="3. 优化器"></a>3. 优化器</h3><ul><li>torch.optim 中提供了众多优化器来计算参数的梯度</li><li>SGD优化器：随机梯度下降，计算输入样本随机子集的平均梯度</li><li>Adam优化器：该优化器自适应调整学习率，且对输入参数的大小不敏感（不需要normalize)</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">t_c = [<span class="number">0.5</span>, <span class="number">14.0</span>, <span class="number">15.0</span>, <span class="number">28.0</span>, <span class="number">11.0</span>, <span class="number">8.0</span>, <span class="number">3.0</span>, -<span class="number">4.0</span>, <span class="number">6.0</span>, <span class="number">13.0</span>, <span class="number">21.0</span>]</span><br><span class="line">t_u = [<span class="number">35.7</span>, <span class="number">55.9</span>, <span class="number">58.2</span>, <span class="number">81.9</span>, <span class="number">56.3</span>, <span class="number">48.9</span>, <span class="number">33.9</span>, <span class="number">21.8</span>, <span class="number">48.4</span>, <span class="number">60.4</span>, <span class="number">68.4</span>]</span><br><span class="line">t_c = torch.tensor(t_c)</span><br><span class="line">t_u = torch.tensor(t_u)</span><br><span class="line"><span class="built_in">dir</span>(optim)  <span class="comment"># 所有优化器都要求输入参数的requires_grad=True</span></span><br><span class="line"><span class="comment"># Using a Gradient Descent Opimizer</span></span><br><span class="line">params = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">learning_rate = <span class="number">1e-5</span></span><br><span class="line">optimizer = optim.SGD([params], lr=learning_rate)  <span class="comment">#SGC stand for stochastic gradient desent, 动量默认为0</span></span><br><span class="line"><span class="comment">#计算输入样本随机子集的平均梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">t_u, w, b</span>):</span><br><span class="line">    <span class="keyword">return</span> w * t_u +b</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_fn</span>(<span class="params">t_p, t_c</span>):</span><br><span class="line">    squared_diffs = (t_p - t_c)**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> squared_diffs.mean()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_loop</span>(<span class="params">n_epochs, optimizer, params, t_u, t_c</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</span><br><span class="line">        t_p = model(t_u, *params)</span><br><span class="line">        loss = loss_fn(t_p, t_c)</span><br><span class="line">        optimizer.zero_grad()<span class="comment"># 梯度归零必须在bakward()之前</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Epoch %d, Loss %f&#x27;</span> % (epoch, <span class="built_in">float</span>(loss)))</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line">params = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = optim.SGD([params], lr=learning_rate)</span><br><span class="line">training_loop(n_epochs=<span class="number">5000</span>, optimizer=optimizer, params=params, t_u = <span class="number">0.1</span> * t_u, t_c = t_c)</span><br><span class="line"><span class="comment"># 尝试使用Adam优化器，该优化器自适应调整学习率，且对输入参数的大小不敏感（不需要normalize)</span></span><br><span class="line">params = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">learning_rate = <span class="number">1e-1</span></span><br><span class="line">optimizer = optim.Adam([params], lr=learning_rate)</span><br><span class="line">training_loop(n_epochs=<span class="number">2000</span>, optimizer=optimizer, params=params, t_u = t_u, t_c = t_c)</span><br></pre></td></tr></table></figure><h3 id="4-训练、验证和过拟合"><a href="#4-训练、验证和过拟合" class="headerlink" title="4. 训练、验证和过拟合"></a>4. 训练、验证和过拟合</h3><ul><li>防止过拟合的方法;1.penalization term, 2.人工增加noise点，3.使用简单模型</li><li>划分数据集和验证集， shuffling</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_samples = t_u.shape[<span class="number">0</span>]</span><br><span class="line">n_val = <span class="built_in">int</span>(<span class="number">0.2</span> * n_samples)</span><br><span class="line">shuffled_indices = torch.randperm(n_samples)</span><br><span class="line">train_indices = shuffled_indices[:-n_val]</span><br><span class="line">val_indices = shuffled_indices[-n_val:]</span><br><span class="line">train_indices, val_indices</span><br><span class="line">train_t_u = t_u[train_indices]</span><br><span class="line">train_t_c = t_c[train_indices]</span><br><span class="line">val_t_u = t_u[val_indices]</span><br><span class="line">val_t_c = t_c[val_indices]</span><br><span class="line">train_t_un = <span class="number">0.1</span> * train_t_u</span><br><span class="line">val_t_un = <span class="number">0.1</span> * val_t_u</span><br><span class="line"><span class="comment"># torch.no_grad() 关闭梯度自动计算， 不构成计算图，节省计算资源</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_loop</span>(<span class="params">n_epochs, optimizer, params, train_t_u, train_t_c, val_t_u, val_t_c</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</span><br><span class="line">        train_t_p = model(train_t_u, *params)</span><br><span class="line">        train_loss = loss_fn(train_t_p, train_t_c)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            val_t_p = model(val_t_u, *params)</span><br><span class="line">            val_loss = loss_fn(val_t_p, val_t_c)</span><br><span class="line">            <span class="keyword">assert</span> val_loss.requires_grad == <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        train_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> epoch &lt;=<span class="number">3</span> <span class="keyword">or</span> epoch % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, Training loss <span class="subst">&#123;train_loss.item():<span class="number">.4</span>f&#125;</span>,&quot;</span></span><br><span class="line">                 <span class="string">f&quot; Validation loss <span class="subst">&#123;val_loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line">params = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = optim.SGD([params], lr=learning_rate)</span><br><span class="line">training_loop(n_epochs=<span class="number">3000</span>, optimizer=optimizer, params=params, </span><br><span class="line">              train_t_u = train_t_un, train_t_c = train_t_c, val_t_u = val_t_un, val_t_c = val_t_c)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A01_Tensor</title>
      <link href="/2022/11/11/A01-Tensor/"/>
      <url>/2022/11/11/A01-Tensor/</url>
      
        <content type="html"><![CDATA[<h2 id="Real-world-data-representation-using-tensors"><a href="#Real-world-data-representation-using-tensors" class="headerlink" title="Real-world data representation using tensors"></a>Real-world data representation using tensors</h2><h3 id="1-working-with-images"><a href="#1-working-with-images" class="headerlink" title="1. working with images"></a>1. working with images</h3><p>在pytorch中，常用imageio库来读取图片，读取函数为<em>imread</em>。读取的图片以Numpy数组的形式储存，即：H*W*C的形式，但Pytorch中需要转换为：C*H*W</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line">img_arr = imageio.imread(<span class="string">&#x27;../data/p1ch4/image-dog/bobby.jpg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(img_arr.shape)</span><br><span class="line">img = torch.from_numpy(img_arr)<span class="comment">#形成张量Tensor</span></span><br><span class="line">out = img.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)<span class="comment">#交换维度</span></span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line">batch_size = <span class="number">3</span></span><br><span class="line">batch = torch.zeros(batch_size, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>, dtype=torch.uint8)<span class="comment">#同时处理多张图片，数据以uint8的形式储存</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">data_dir = <span class="string">&#x27;../data/p1ch4/image-cats/&#x27;</span><span class="comment">#设定相对路径</span></span><br><span class="line"><span class="comment"># 读取文件名，储存成字符串数组</span></span><br><span class="line">filenames = [name <span class="keyword">for</span> name <span class="keyword">in</span> os.listdir(data_dir)</span><br><span class="line"><span class="keyword">if</span> os.path.splitext(name)[-<span class="number">1</span>] == <span class="string">&#x27;.png&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i, filename <span class="keyword">in</span> <span class="built_in">enumerate</span>(filenames):</span><br><span class="line">    img_arr = imageio.imread(os.path.join(data_dir, filename))<span class="comment">#获取绝对路径</span></span><br><span class="line">    img_t = torch.from_numpy(img_arr)<span class="comment">#生成tensor</span></span><br><span class="line">    img_t = img_t.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    img_t = img_t[:<span class="number">3</span>]  <span class="comment">#此处仅保留前3个通道，图片有可能有其他通道来表示透明度</span></span><br><span class="line">    batch[i] = img_t</span><br><span class="line"><span class="comment"># 正则化，使其投影到（-1，1）区间，使数变小提升训练的性能</span></span><br><span class="line">batch = batch.<span class="built_in">float</span>()</span><br><span class="line">batch /= <span class="number">255.0</span></span><br><span class="line">n_channels = batch.shape[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(n_channels):</span><br><span class="line">    mean = torch.mean(batch[:, c])</span><br><span class="line">    std = torch.std(batch[:, c])</span><br><span class="line">    batch[:, c] = (batch[:, c] - mean) /std</span><br></pre></td></tr></table></figure><h3 id="2-3D-images-Volumetric-data"><a href="#2-3D-images-Volumetric-data" class="headerlink" title="2. 3D images: Volumetric data"></a>2. 3D images: Volumetric data</h3><p>对于tensor来说，体积数据于图片数据区别不大，仅多了一个维度，其通道为：N*C*D*H*W。使用imageio库中的volread函数进行读取，下面将读取三维切片数据。RGB表示一张图像，而切片数据由多张图像组成，每张图像都有自己的RGB。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line">dir_path = <span class="string">&quot;../data/p1ch4/volumetric-dicom/2-LUNG 3.0 B70f-04083&quot;</span></span><br><span class="line">vol_arr = imageio.volread(dir_path, <span class="string">&#x27;DICOM&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(vol_arr.shape)</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">vol = torch.from_numpy(vol_arr).<span class="built_in">float</span>()</span><br><span class="line">vol = torch.unsqueeze(vol, <span class="number">0</span>)<span class="comment">#扩充一个维度，用于存放通道</span></span><br><span class="line"><span class="built_in">print</span>(vol.shape)</span><br></pre></td></tr></table></figure><h3 id="3-Representing-tabular-data"><a href="#3-Representing-tabular-data" class="headerlink" title="3. Representing tabular data"></a>3. Representing tabular data</h3><ul><li><p>机器学习中会遇到的最简单的数据结构形式，spreadsheet、CSV、database</p></li><li><p>python中提供了3个常用加载.csv文件的方法：1.csv module, 2.Numpy, 3.Pandas（其中Pandas是最快、最节省内存的）</p></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">wine_path = <span class="string">&quot;../data/p1ch4/tabular-wine/winequality-white.csv&quot;</span></span><br><span class="line">wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter =<span class="string">&quot;;&quot;</span>, skiprows=<span class="number">1</span>)<span class="comment">#skiprows跳过表头</span></span><br><span class="line"><span class="built_in">print</span>(wineq_numpy)</span><br><span class="line"><span class="comment"># open() 函数用于打开一个文件，创建一个file对象，相关的方法才可以调用它进行读写</span></span><br><span class="line"><span class="comment"># csv.reader()返回一个reader对象，利用该对象遍历csv文件中的行</span></span><br><span class="line"><span class="comment"># next()返回迭代器的下一行</span></span><br><span class="line">col_list = <span class="built_in">next</span>(csv.reader(<span class="built_in">open</span>(wine_path), delimiter=<span class="string">&#x27;;&#x27;</span>))<span class="comment">#返回表头</span></span><br><span class="line"><span class="built_in">print</span>(wineq_numpy.shape, col_list)</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">wineq = torch.from_numpy(wineq_numpy)</span><br><span class="line"><span class="built_in">print</span>(wineq.shape, wineq.dtype)</span><br><span class="line"><span class="comment"># 分别读取数据和标签</span></span><br><span class="line">data = wineq[:, :-<span class="number">1</span>]</span><br><span class="line">target = wineq[:, -<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(data, data.shape)</span><br><span class="line"><span class="built_in">print</span>(target, target.shape)</span><br><span class="line"><span class="comment"># 将目标张量转化成标签张量，将值转化为整数处理</span></span><br><span class="line">target = wineq[:, -<span class="number">1</span>].long()</span><br><span class="line"><span class="built_in">print</span>(target.shape)</span><br><span class="line"><span class="comment"># one-hot编码</span></span><br><span class="line">target_onehot = torch.zeros(target.shape[<span class="number">0</span>], <span class="number">10</span>)<span class="comment"># 创建一个空的tensor，用于存放编码</span></span><br><span class="line"><span class="comment"># 调用scatter_()函数，直接对目标对象进行修改</span></span><br><span class="line"><span class="comment"># 第一个1表示对第2（从0开始）个维度进行修改，第二个1.0表示用1填充对应位置</span></span><br><span class="line"><span class="comment"># target.unsqueeze(1)用于索引修改元素的位置，unsqueeze(num)函数表示在num处扩充一列</span></span><br><span class="line">target_onehot.scatter_(<span class="number">1</span>, target.unsqueeze(<span class="number">1</span>), <span class="number">1.0</span>)</span><br><span class="line"><span class="built_in">print</span>(target_onehot.shape)</span><br></pre></td></tr></table></figure><h3 id="4-时间序列"><a href="#4-时间序列" class="headerlink" title="4. 时间序列"></a>4. 时间序列</h3><ul><li>常需要把字符串的时间日期转换成数字</li><li>one-hot编码和.cat()函数拼接tensor，one-hot编码对于表示分类数据而言是一项非常好的技术</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">bikes_numpy = np.loadtxt(<span class="string">&quot;../data/p1ch4/bike-sharing-dataset/hour-fixed.csv&quot;</span>,</span><br><span class="line">                        dtype=np.float32,</span><br><span class="line">                        delimiter=<span class="string">&quot;,&quot;</span>,</span><br><span class="line">                        skiprows=<span class="number">1</span>,</span><br><span class="line">                        converters=&#123;<span class="number">1</span>: <span class="keyword">lambda</span> x: <span class="built_in">float</span>(x[<span class="number">8</span>:<span class="number">10</span>])&#125;)  <span class="comment"># 将字符串数据转化成对应的数字</span></span><br><span class="line">bikes = torch.from_numpy(bikes_numpy)</span><br><span class="line"><span class="built_in">print</span>(bikes)</span><br><span class="line"><span class="built_in">print</span>(bikes.shape, bikes.stride())</span><br><span class="line">daily_bikes = bikes.view(-<span class="number">1</span>, <span class="number">24</span>, bikes.shape[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(daily_bikes.shape, daily_bikes.stride())</span><br><span class="line">daily_bikes = daily_bikes.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">daily_bikes.shape, daily_bikes.stride()</span><br><span class="line">first_day = bikes[:<span class="number">24</span>].long()</span><br><span class="line"><span class="comment"># one-hot编码</span></span><br><span class="line">weather_onehot = torch.zeros(first_day.shape[<span class="number">0</span>], <span class="number">4</span>)</span><br><span class="line">first_day[:, <span class="number">9</span>], first_day.shape</span><br><span class="line">weather_onehot.scatter_(<span class="number">1</span>, first_day[:, <span class="number">9</span>].unsqueeze(<span class="number">1</span>).long() - <span class="number">1</span>, <span class="number">1.0</span>)</span><br><span class="line"><span class="comment"># 将两个tensor沿着dim维度进行拼接</span></span><br><span class="line">torch.cat((bikes[:<span class="number">24</span>], weather_onehot), <span class="number">1</span>)[:<span class="number">2</span>]</span><br><span class="line">daily_weather_onehot = torch.zeros(daily_bikes.shape[<span class="number">0</span>], <span class="number">4</span>,daily_bikes.shape[<span class="number">2</span>])</span><br><span class="line">daily_weather_onehot.shape</span><br><span class="line">daily_weather_onehot.scatter_(<span class="number">1</span>, daily_bikes[:,<span class="number">9</span>,:].long().unsqueeze(<span class="number">1</span>) - <span class="number">1</span>, <span class="number">1.0</span>)</span><br><span class="line">daily_weather_onehot.shape</span><br><span class="line">daily_bikes = torch.cat((daily_bikes, daily_weather_onehot), dim=<span class="number">1</span>)</span><br><span class="line">daily_bikes[:, <span class="number">9</span>, :] = (daily_bikes[:, <span class="number">9</span>, :] - <span class="number">1.0</span>) / <span class="number">3.0</span></span><br><span class="line">tem = daily_bikes[:, <span class="number">10</span>, :]</span><br><span class="line">tem_min = torch.<span class="built_in">min</span>(tem)</span><br><span class="line">tem_max = torch.<span class="built_in">max</span>(tem)</span><br><span class="line">daily_bikes[:, <span class="number">10</span>, :] = (daily_bikes[:, <span class="number">10</span>, :] - tem_min) / (tem_max - tem_min)</span><br><span class="line"><span class="comment"># 或者使用平均值和方差来归一化</span></span><br></pre></td></tr></table></figure><h3 id="5-文本"><a href="#5-文本" class="headerlink" title="5. 文本"></a>5. 文本</h3><ul><li>首先将文本转化成数字，有两个层面：1.字符层面；2.单词（字符串）层面。都是使用one-hot编码实现</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取文本文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;../data/p1ch4/jane-austen/1342-0.txt&quot;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    text = f.read()</span><br><span class="line"><span class="comment"># 进行one-hot编码字符，单词</span></span><br><span class="line">lines = text.split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">line = lines[<span class="number">200</span>]</span><br><span class="line">line</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">letter_t = torch.zeros(<span class="built_in">len</span>(line), <span class="number">128</span>)</span><br><span class="line">letter_t.shape</span><br><span class="line"><span class="keyword">for</span> i, letter <span class="keyword">in</span> <span class="built_in">enumerate</span>(line.lower().strip()):</span><br><span class="line">    letter_index = <span class="built_in">ord</span>(letter) <span class="keyword">if</span> <span class="built_in">ord</span>(letter) &lt; <span class="number">128</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    letter_t[i][letter_index] = <span class="number">1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean_words</span>(<span class="params">input_str</span>):</span><br><span class="line">    punctuation = <span class="string">&#x27;.,;:！？“”-_&#x27;</span></span><br><span class="line">    word_list = input_str.lower().replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27; &#x27;</span>).split( )</span><br><span class="line">    word_list = [word.strip(punctuation) <span class="keyword">for</span> word <span class="keyword">in</span> word_list]   <span class="comment"># .strip(xx)函数为除去xx元素</span></span><br><span class="line">    <span class="keyword">return</span> word_list</span><br><span class="line">words_in_line = clean_words(line)</span><br><span class="line">line, words_in_line</span><br><span class="line"><span class="comment"># 在单词和编码之间建立映射关系</span></span><br><span class="line">word_list = <span class="built_in">sorted</span>(<span class="built_in">set</span>(clean_words(text)))</span><br><span class="line">word2index_dict = &#123;word: i <span class="keyword">for</span> (i, word) <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list)&#125;</span><br><span class="line"><span class="built_in">len</span>(word2index_dict), word2index_dict[<span class="string">&#x27;impossible&#x27;</span>]</span><br><span class="line">word_t = torch.zeros(<span class="built_in">len</span>(words_in_line), <span class="built_in">len</span>(word2index_dict))</span><br><span class="line"><span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(words_in_line):</span><br><span class="line">    word_index = word2index_dict[word]</span><br><span class="line">    word_t[i][word_index] = <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&#123;:2&#125; &#123;:4&#125; &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i, word_index, word))</span><br><span class="line"><span class="built_in">print</span>(word_t.shape)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tensor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1111-反思</title>
      <link href="/2022/11/10/1111-%E5%8F%8D%E6%80%9D/"/>
      <url>/2022/11/10/1111-%E5%8F%8D%E6%80%9D/</url>
      
        <content type="html"><![CDATA[<p>最近断更了，以下是自我反思。</p><p>自从开始准备博资考以来，博客这一块被我忽略了。在面对一件事或者说一场考试的时候，还是容易慌张，过渡的紧张，虽然知道应该怎么应对，但是还是不能做到这个年纪该有的沉稳。也许这就是读博长期处于学校的封闭环境中所导致的吧，还是不能够经事。希望后面要注意锻炼，以兵来将挡、水来土掩的态度面对生活给我们出的难题。</p><p>跑偏了，博资考并不是断更的主要原因。主要还是有些迷茫了，心态有了些许变化，还有部分原因是经常感到疲惫，不知道是不是减肥吃的少导致体力有些跟不上了。虽然课题组最近比较忙，但是回宿舍的时间基本不变，要抽时间学习还是能做到的。从心态开始说吧，首先学习深度学习这个知识到底对我博士课题有没有左右这一点越来越模糊。按照前几次与导师交流的情况来看，他还是希望博士论文中出现这部分内容，但在深入学习过后，越来越找不到可应用的点，这导致学习的积极性有所下降。其次，就是学习方法可能也出现了一些问题，在学习过程中没有获得正反馈，一直是高强度输入，这也是导致学习效率不高的应该原因。心态上还有一个大的问题就是，有些好高骛远，什么都想学习，比如学英语、深度学习、CFD、C++、看课外书等等，这些都是国外博士在科研中必须掌握的技能，只是没有做好时间安排和规划，甚至于有些人云亦云的感觉在里面。因此，希望后面能够在合理安排自己娱乐时间的基础上，对以上的学习做一个清晰的规划。最后，就是最近经常感到很疲惫，睡眠质量还超级好….。是因为吃得少了能量不够吗？不管怎么样体重是要维持到115斤的，同时坚持做一些运动让自己储备一些体力吧。</p><p>我心知肚明的是：想做成的事，就要立即去尝试，不需要再准备过程花费太多的时间，JUST DO IT！</p><p>同时，在生活中一定要克制好自己的情绪，没必要对别人一脸严肃，大家都不容易。要做到：严于律己，宽以待人！</p><ul><li><input disabled="" type="checkbox"> 在今年结束的时候，我希望在博客里更新一个对今年的总结与回顾</li></ul>]]></content>
      
      
      <categories>
          
          <category> 科研总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 心得体会 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>师兄博士毕业经验总结</title>
      <link href="/2022/10/04/%E5%B8%88%E5%85%84%E5%8D%9A%E5%A3%AB%E6%AF%95%E4%B8%9A%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"/>
      <url>/2022/10/04/%E5%B8%88%E5%85%84%E5%8D%9A%E5%A3%AB%E6%AF%95%E4%B8%9A%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h3 id="师兄博士毕业经验总结"><a href="#师兄博士毕业经验总结" class="headerlink" title="师兄博士毕业经验总结"></a>师兄博士毕业经验总结</h3><hr><h4 id="1-刘博士预答辩总结220330"><a href="#1-刘博士预答辩总结220330" class="headerlink" title="1 刘博士预答辩总结220330"></a>1 刘博士预答辩总结220330</h4><ul><li>汇报PPT需要简洁美观，颜色搭配合理</li><li>创新点：<ul><li>至少3个以上创新点，且需要有相应学术成果支撑</li><li>从实验中提炼有价值的东西，返回到解决的科学问题上，对…提供依据和参考</li></ul></li><li>文献调研：<ul><li>体现学术价值和学术性的地方，需要严谨且全面</li><li>需要确定研究的应用背景、对象以及使用指标</li></ul></li><li>研究内容：<ul><li>博士论文理论方面必须充分，体现深度</li><li>文章内容的深度可以体现在充分的受力分析上</li><li>每章标题的选择要和文献研究现状对应上</li><li>研究中所用值或参数必须有所依据，用词要准确</li></ul></li><li>结论：<ul><li>文章得出的结论需要有学术价值或应用价值</li><li>明确试验所得结果和研究所得结论，区分猜测和推测</li></ul></li></ul><h4 id="2-刘博士学位论文答辩2205119"><a href="#2-刘博士学位论文答辩2205119" class="headerlink" title="2 刘博士学位论文答辩2205119"></a>2 刘博士学位论文答辩2205119</h4><ul><li>答辩目的：考察学生的思考能力与回答问题的能力，及时应变能力以及思辨能力</li><li>提出的建议：<ul><li>可视化研究后，依据结果进行建模，将模型用于LHP的设计，再次进行加工，最后在实验中得到验证（将倾角考虑到理论模型中）</li><li>可视化结果应该尽量展示原始图片</li><li>文献综述必须完备，从外行角度看起来齐全（从大类开始介绍，逐渐缩小至研究对象）</li><li>工质的选择过程需要补全</li><li>针对加速度条件，希望得出综合的临界判据（无量纲参数）</li></ul></li><li>提出的问题：<ul><li>将加速度考虑为额外的力，而不是流阻？</li><li>为什么是论文中的这个倾角？能否采取一定措施对一些不利的现象进行抑制，推广其使用</li><li>加速度大小为什么有时是7 g，有时是15 g？</li><li>能否考虑增加毛细结构（辅芯）后，在加速度下的工作情况？</li><li>多考虑其他参数？</li></ul></li></ul><h4 id="3-唐博士组内答辩"><a href="#3-唐博士组内答辩" class="headerlink" title="3 唐博士组内答辩"></a>3 唐博士组内答辩</h4><ul><li><p>所提意见：</p><ul><li>研究前需要明确，所研究的一些特殊现象是否值得研究</li><li>做了理论和实验的研究，最好能够有一定的实验或者仿真来进行验证</li><li>做研究过程中遇到困难和问题不能拐弯，必须迎难而上</li></ul></li><li><p>存在不足：</p><ul><li>研究现状到研究内容的过渡，对现有研究不足的总结十分重要，要凝练突出需要解决的问题</li><li>每章的结束要有结论性的体现</li><li>要突出自己的成果和独特之处，不能表现过于平淡，要学会适当进行拔高，把自己的工作和特点体现出来</li><li>前言中，需要把理由、要解决的问题及要达到的高度体现出来</li><li>每个章节的题目需要反复推敲，逻辑严密</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 科研总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 经验总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高算平台使用</title>
      <link href="/2022/10/03/%E9%AB%98%E7%AE%97%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8/"/>
      <url>/2022/10/03/%E9%AB%98%E7%AE%97%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="高算平台使用"><a href="#高算平台使用" class="headerlink" title="高算平台使用"></a>高算平台使用</h2><p>使用前需要仔细阅读官方使用文档（用户手册），可见<code>/gs/doc</code>。为了课题组内同学学习，根据组内项目常用计算需求，做了如下整理：</p><h3 id="1-平台介绍"><a href="#1-平台介绍" class="headerlink" title="1. 平台介绍"></a>1. 平台介绍</h3><ul><li><p>操作系统：CentiOS 7.6，作业调度：Slurm 19.04</p></li><li><p>计算节点分为分为：GPU计算节点、CPU计算节点</p></li><li><p>账号统一管理，学生子账号归属于导师账号，每个账号默认提供2TB存储空间（超过后文件读写会报错）</p></li><li><p>软件：Ansys2019R2、OpenFoam5&#x2F;6、Matlab等（不支持图形界面），<em>也可以在自己下编译安装其他软件</em></p></li><li><p>CPU收费标准：</p><table><thead><tr><th>服务质量</th><th>单价（元&#x2F;核时）</th><th>最大运行时长</th></tr></thead><tbody><tr><td>cpu-low</td><td>0.05</td><td>7</td></tr><tr><td>cpu-normal</td><td>0.07</td><td>7</td></tr><tr><td>cpu-high</td><td>0.1</td><td>7</td></tr><tr><td>cpu-quota（科研优先，需申请使用）</td><td>0.05</td><td>7</td></tr></tbody></table><p><em>注：各分区计算速度没有区别，只是在平台任务较多时，排队优先级较高，能够优先分配到计算资源。</em></p></li></ul><h3 id="2-平台访问"><a href="#2-平台访问" class="headerlink" title="2. 平台访问"></a>2. 平台访问</h3><ul><li><p>远程访问系统：使用ssh协议登录，主机选择下述IP地址，端口选择22，输入用户名和密码（<em>申请时邮件中有</em>）即可登录</p></li><li><p>登录，连接系统：</p><ul><li><p>软件：Xshell、MobaXterm、PuTTY、SecureCRT中的随意一种，Xshell可在官网用学校邮箱注册下载免费版</p></li><li><p>登录节点IP地址：<code>10.212.66.4/10.212.66.5/10.212.70.128</code></p></li><li><p>首次登录时，根据提示更改自己账号的密码：**<code>yppasswd</code>**</p></li><li><p>远程登陆时，需要使用堡垒机，登录网址：bhblj.buaa.edu.cn，具体使用方法可找高算中心询问</p></li><li><p>登录后，您将在自己账号目录下进行工作，&#x2F;gs&#x2F;home&#x2F;yourID</p></li><li><p>10次密码错误登录，电脑IP将被封锁，需要联系中心技术人员解锁</p></li></ul></li><li><p>文件传输：</p><ul><li>软件：Xftp，可在官网用学校邮箱注册下载免费版</li><li>使用：在Xshell或者其他连接软件中，点击Xftp图标即可</li></ul></li></ul><h3 id="2-常用Linux命令"><a href="#2-常用Linux命令" class="headerlink" title="2.常用Linux命令"></a>2.常用Linux命令</h3><p>​学习常用Linux命令可以帮助快速管理文件即任务，也是软件编译安装的必备知识，可参考[<a href="https://www.runoob.com/linux/linux-tutorial.html">Linux 教程 | 菜鸟教程 (runoob.com)</a>]。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ls列出目录及文件名</span><br><span class="line">cd切换目录</span><br><span class="line">pwd显示当前目录</span><br><span class="line">mkdir创建新目录</span><br><span class="line">rmdir删除空目录</span><br><span class="line">cp复制文件或目录</span><br><span class="line">rm删除文件或目录</span><br><span class="line">mv移动文件与目录，或者修改文件与目录名</span><br></pre></td></tr></table></figure><h3 id="3-任务提交"><a href="#3-任务提交" class="headerlink" title="3.任务提交"></a>3.任务提交</h3><p>任务提交有两种方式：使用Slurm作业调度系统、交互式作业提交</p><ul><li><p>提交文件：需要准备xxx.slurm、inputfile文本文件和所需的计算文件（fluent的cas和dat文件）</p><ul><li><p>xxx.slurm文件：批处理文件，用于作业调度，包括资源分配、运行时间、运行环境加载（module）</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">#SBATCH -J xxx              # 作业名是xxx，可随意命名</span><br><span class="line">#SBATCH -p cpu-low               # 提交到low分区</span><br><span class="line">#SBATCH -N 1                 # 使用1个节点</span><br><span class="line">#SBATCH -n 36                 # 使用36个CPU核</span><br><span class="line">#SBATCH --ntasks-per-node 36    # 每个节点使用36个CPU核</span><br><span class="line">#SBATCH -t 4-10:30:00              # 任务最大运行时间是4天10小时30分钟</span><br><span class="line">#SBATCH -o fluent.out          # 将屏幕的输出结果保存到当前文件夹的fluent.out</span><br><span class="line">#SBATCH -e fluent.err          # 将屏幕的错误输出结果保存到当前文件夹的fluent.err</span><br><span class="line"></span><br><span class="line">srun hostname | sort &gt; machinefile.$&#123;SLURM_JOB_ID&#125;</span><br><span class="line"></span><br><span class="line">NP=`cat machinefile.$&#123;SLURM_JOB_ID&#125; | wc -l`</span><br><span class="line"></span><br><span class="line">mech_host=&quot;&quot;</span><br><span class="line">for host in ` sort -u machinefile.$&#123;SLURM_JOB_ID&#125;`;do</span><br><span class="line">  n=$(grep -c $host machinefile.$&#123;SLURM_JOB_ID&#125;)</span><br><span class="line">        mech_host=$(echo &quot;$host:$n,$mech_host&quot;)</span><br><span class="line">done</span><br><span class="line">mech_host=$(echo $mech_host|sed &quot;s/,$//&quot;)</span><br><span class="line">echo $mech_host</span><br><span class="line"></span><br><span class="line">module load ansys/v192</span><br><span class="line">module load intel/18.0.3.222</span><br><span class="line"></span><br><span class="line">fluent 3d -g -ssh -pib -mpi=intel -t$NP  -cnf=&quot;$mech_host&quot; -i inputfile#打开fluent(3d模式)，读取inputfile</span><br></pre></td></tr></table></figure></li><li><p>inputfile文件：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/file/read-case TJ_32.5w.cas</span><br><span class="line">/file/read-data TJ_32.5w.dat</span><br><span class="line">/file/autosave/data-frequency 500</span><br><span class="line">/file/autosave/root-name TJ_32.5w</span><br><span class="line">/solve/dual-time-iterate 10000 40</span><br><span class="line">/solve/</span><br><span class="line">/file/write-data TJ_32.5w-end</span><br><span class="line">/exit</span><br><span class="line">yes</span><br></pre></td></tr></table></figure></li></ul></li><li><p>提交任务命令：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sbatch xxx.slurm# 使用Slurm作业调度系统，使用简单</span><br><span class="line">srun [options] command# 使用交互式作业提交，在[options]和command中设置上述文件内容</span><br><span class="line"># 交互式作业提交、Slurm并行、串行作业提交可参考 /gs/doc下的用户手册</span><br></pre></td></tr></table></figure></li><li><p>Fluent常用TUI命令：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/solve/interate N#稳态计算，N为迭代步数</span><br><span class="line">/solve/dual-time-interate N1 N2#N1为时间步数，N2为单个时间步内的最大迭代次数</span><br><span class="line"># 其余TUI命令可以在fluent的consol中自行摸索</span><br></pre></td></tr></table></figure></li></ul><h3 id="4-任务状态查询与操作"><a href="#4-任务状态查询与操作" class="headerlink" title="4.任务状态查询与操作"></a>4.任务状态查询与操作</h3><ul><li><p>提交作业前，查看可用资源使用：**<code>sinfo</code>**，状态为idle或mix表示空闲</p></li><li><p>状态查询：**<code>squeue</code>**</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">squeue# 返回所有人的任务信息</span><br><span class="line">squeue -u yourID     #返回自己的任务信息,yourID为自己的用户名</span><br></pre></td></tr></table></figure><p><em>其中，状态R表示运行中，PD表示排队，TD表示超时，S表示挂起，CG表示退出，CD表示成功结束。REASON会提示具体原因</em></p></li><li><p>结束作业：scancel</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scancel jobID#jobID为提交时返回的任务号，如7953358</span><br></pre></td></tr></table></figure></li><li><p>追踪作业：scontrol</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scontrol show job jobID#jobID为提交时返回的任务号，如7953358</span><br></pre></td></tr></table></figure></li><li><p>查询历史任务：sacct</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sacct#会返回当天的所有作业</span><br><span class="line">sacct -S MMDD#返回从MM月DD日到现在的所有历史作业</span><br></pre></td></tr></table></figure></li><li><p>更新作业：在任务排队过程中，对任务信息进行修改</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scontrol update jobid=jobID ...</span><br></pre></td></tr></table></figure></li></ul><h3 id="5-账号申请与使用"><a href="#5-账号申请与使用" class="headerlink" title="5. 账号申请与使用"></a>5. 账号申请与使用</h3><ul><li><p>账号申请（导师OA）：<a href="http://nic.buaa.edu.cn/content.jsp?urltype=news.NewsContentUrl&amp;wbtreeid=1025&amp;wbnewsid=3486">http://nic.buaa.edu.cn/content.jsp?urltype=news.NewsContentUrl&amp;wbtreeid=1025&amp;wbnewsid=3486</a></p></li><li><p>缴费（导师OA）：<a href="http://nic.buaa.edu.cn/content.jsp?urltype=news.NewsContentUrl&amp;wbtreeid=1025&amp;wbnewsid=3487">http://nic.buaa.edu.cn/content.jsp?urltype=news.NewsContentUrl&amp;wbtreeid=1025&amp;wbnewsid=3487</a></p></li><li><p>剩余费用查询（使用导师账号登录）：<a href="http://10.212.66.3/">http://10.212.66.3</a></p></li><li><p>最小计费单元0.01小时，即提交一次任务分配到资源后运行失败也算0.01小时</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 科研总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高算平台 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>12-RNN-Classifer</title>
      <link href="/2022/09/30/12-RNN-Classifer/"/>
      <url>/2022/09/30/12-RNN-Classifer/</url>
      
        <content type="html"><![CDATA[<p>学习笔记：处理自然语言的方法和流程</p><h3 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a>问题引入</h3><ul><li>根据几千个来自18个不同语言地区的人名，训练一个能够根据人名预测地区的模型</li><li>输入的名字为一个序列，每一个字母都是一个x，用最终的隐层输出来判断属于什么类别</li></ul><h3 id="使用模型"><a href="#使用模型" class="headerlink" title="使用模型"></a>使用模型</h3><ul><li><p>输入&gt;嵌入层&gt;GRU层&gt;线性层&gt;输出</p></li><li><p>双向循环神经网络：将前向和后向计算的隐层输出做拼接，$hidden &#x3D; [h_N^f, h_N^b]$</p></li><li><p>全部代码实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># Paramenters</span></span><br><span class="line">HIDDEN_SIZE = <span class="number">100</span></span><br><span class="line">BATCH_SIZE = <span class="number">256</span></span><br><span class="line">N_LAYER = <span class="number">2</span></span><br><span class="line">N_EPOCHS = <span class="number">100</span></span><br><span class="line">N_CHARS = <span class="number">128</span></span><br><span class="line">USE_GPU = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare dataset</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NameDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, is_train_set=<span class="literal">True</span></span>):</span><br><span class="line">        filename = <span class="string">&#x27;names_train.csv.gz&#x27;</span> <span class="keyword">if</span> is_train_set <span class="keyword">else</span> <span class="string">&#x27;names_test.csv.gz&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> gzip.<span class="built_in">open</span>(filename, <span class="string">&#x27;rt&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            reader = csv.reader(f)</span><br><span class="line">            rows = <span class="built_in">list</span>(reader)</span><br><span class="line">        self.names = [row[<span class="number">0</span>] <span class="keyword">for</span> row <span class="keyword">in</span> rows]</span><br><span class="line">        self.<span class="built_in">len</span> = <span class="built_in">len</span>(self.names)</span><br><span class="line">        self.countries = [row[<span class="number">1</span>] <span class="keyword">for</span> row <span class="keyword">in</span> rows]</span><br><span class="line">        self.country_list = <span class="built_in">list</span>(<span class="built_in">sorted</span>(<span class="built_in">set</span>(self.countries)))</span><br><span class="line">        self.country_dict = self.getCountryDict()</span><br><span class="line">        self.country_num = <span class="built_in">len</span>(self.country_list)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.names[index], self.country_dict[self.countries[index]]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getCountryDict</span>(<span class="params">self</span>):</span><br><span class="line">        country_dict = <span class="built_in">dict</span>()</span><br><span class="line">        <span class="keyword">for</span> idx, country_name <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.country_list, <span class="number">0</span>):</span><br><span class="line">            country_dict[country_name] = idx</span><br><span class="line">        <span class="keyword">return</span> country_dict</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">idx2country</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.country_list[index]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getCountriesNum</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.country_num</span><br><span class="line">trainset = NameDataset(is_train_set=<span class="literal">True</span>)</span><br><span class="line">trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line">testset = NameDataset(is_train_set=<span class="literal">False</span>)</span><br><span class="line">testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">N_COUNTRY = trainset.getCountriesNum()</span><br><span class="line"><span class="comment"># Design Model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNClassifier</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size, n_layers=<span class="number">1</span>, bidirectional=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNClassifier, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.n_directions = <span class="number">2</span> <span class="keyword">if</span> bidirectional <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        self.embedding = torch.nn.Embedding(input_size, hidden_size)</span><br><span class="line">        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers, bidirectional=bidirectional)</span><br><span class="line">        self.fc = torch.nn.Linear(hidden_size * self.n_directions, output_size)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_hidden</span>(<span class="params">self, batch_size</span>):</span><br><span class="line">        hidden = torch.zeros(self.n_layers * self.n_directions, batch_size, self.hidden_size)</span><br><span class="line">        <span class="keyword">return</span> create_tensor(hidden)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, seq_length</span>):</span><br><span class="line">        <span class="comment"># 对input进行转置</span></span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.t()</span><br><span class="line">        batch_size = <span class="built_in">input</span>.size(<span class="number">1</span>)</span><br><span class="line">        hidden = self._init_hidden(batch_size)</span><br><span class="line">        embedding = self.embedding(<span class="built_in">input</span>)</span><br><span class="line">        <span class="comment">#对数据计算过程提速</span></span><br><span class="line">        <span class="comment"># 需要得到嵌入层的结果（输入数据）以及每条输入数据的长度</span></span><br><span class="line">        gru_input = pack_padded_sequence(embedding, seq_length)</span><br><span class="line">        output, hidden = self.gru(gru_input, hidden)</span><br><span class="line">        <span class="comment"># 如果是双向神经网络会有h_N^f以及h_1^b两个hidden</span></span><br><span class="line">        <span class="keyword">if</span> self.n_directions == <span class="number">2</span>:</span><br><span class="line">            hidden_cat = torch.cat([hidden[-<span class="number">1</span>], hidden[-<span class="number">2</span>]], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hidden_cat = hidden[-<span class="number">1</span>]</span><br><span class="line">        fc_output = self.fc(hidden_cat)</span><br><span class="line">        <span class="keyword">return</span> fc_output</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">name2list</span>(<span class="params">name</span>):</span><br><span class="line">    arr = [<span class="built_in">ord</span>(c) <span class="keyword">for</span> c <span class="keyword">in</span> name]</span><br><span class="line">    <span class="keyword">return</span> arr, <span class="built_in">len</span>(arr)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_tensor</span>(<span class="params">tensor</span>):</span><br><span class="line">    <span class="keyword">if</span> USE_GPU:</span><br><span class="line">        device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        tensor = tensor.to(device)</span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_tensors</span>(<span class="params">names, counteries</span>):</span><br><span class="line">    sequences_and_length = [name2list(name) <span class="keyword">for</span> name <span class="keyword">in</span> names]</span><br><span class="line">    name_sequences = [s1[<span class="number">0</span>] <span class="keyword">for</span> s1 <span class="keyword">in</span> sequences_and_length]</span><br><span class="line">    seq_length = torch.LongTensor([s1[<span class="number">1</span>] <span class="keyword">for</span> s1 <span class="keyword">in</span> sequences_and_length])</span><br><span class="line">    counteries = counteries.long()</span><br><span class="line">    seq_tensor = torch.zeros(<span class="built_in">len</span>(name_sequences), seq_length.<span class="built_in">max</span>()).long()</span><br><span class="line">    <span class="keyword">for</span> idx, (seq, seq_len) <span class="keyword">in</span> <span class="built_in">enumerate</span> (<span class="built_in">zip</span>(name_sequences, seq_length)):</span><br><span class="line">        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)</span><br><span class="line">    seq_length, perm_idx = seq_length.sort(dim=<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">    seq_tensor = seq_tensor[perm_idx]</span><br><span class="line">    countries = counteries[perm_idx]</span><br><span class="line">    <span class="keyword">return</span> create_tensor(seq_tensor),\</span><br><span class="line">           create_tensor(seq_length),\</span><br><span class="line">           create_tensor(countries)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train and test</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">trainModel</span>():</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, (names, countries) <span class="keyword">in</span> <span class="built_in">enumerate</span> (trainloader, <span class="number">1</span>):</span><br><span class="line">        inputs, seq_lengths, target = make_tensors(names, countries)</span><br><span class="line">        output = classifier(inputs, seq_lengths)</span><br><span class="line">        loss  = criterion(output, target)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;[<span class="subst">&#123;time_since(start)&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> &#x27;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;[<span class="subst">&#123;i * <span class="built_in">len</span>(inputs)&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(train_set)&#125;</span>]&#x27;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;loss=<span class="subst">&#123;total_loss / (i * <span class="built_in">len</span>(inputs))&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">testModel</span>():</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="built_in">len</span>(testset)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;evaluating trained model ...&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, (names, countries) <span class="keyword">in</span> <span class="built_in">enumerate</span>(testloader, <span class="number">1</span>):</span><br><span class="line">            inputs, seq_lengths, target = make_tensors(names, countries)</span><br><span class="line">            output = classifier(inputs, seq_lengths)</span><br><span class="line">            pred = output.<span class="built_in">max</span>(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>]</span><br><span class="line">            correct += pred.eq(target.view_as(pred)).<span class="built_in">sum</span>().item()</span><br><span class="line">        percent = <span class="string">&#x27;%.2f&#x27;</span> % (<span class="number">100</span> * correct / total)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Test set: Accuracy <span class="subst">&#123;correct&#125;</span>/<span class="subst">&#123;total&#125;</span> <span class="subst">&#123;percent&#125;</span>%&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> correct / total</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">time_since</span>(<span class="params">since</span>):</span><br><span class="line">    s = time.time() - since</span><br><span class="line">    m = matn.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;%dm %ds&#x27;</span> % (m, s)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)</span><br><span class="line">    <span class="keyword">if</span> USE_GPU:</span><br><span class="line">        device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        classifier.to(device)</span><br><span class="line"></span><br><span class="line">    criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(classifier.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training for %d epochs ...&quot;</span> % N_EPOCHS)</span><br><span class="line">    acc_list = []</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N_EPOCHS+<span class="number">1</span>):</span><br><span class="line">        trainModel()</span><br><span class="line">        acc = testModel()</span><br><span class="line">        acc_list.append(acc)</span><br><span class="line">    epoch = np.arange(<span class="number">1</span>, <span class="built_in">len</span>(acc_list)+<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    acc_list = np.array(acc_list)</span><br><span class="line">    plt.plot(epoch, acc_list)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></li></ul><h3 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h3><ul><li>kaggle上的电影评论：Rotten Tomatoes</li><li>太难了不会，RNN先搁置了吧…..</li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 刘二大人学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11-Basic-RNN</title>
      <link href="/2022/09/27/11-Basic-RNN/"/>
      <url>/2022/09/27/11-Basic-RNN/</url>
      
        <content type="html"><![CDATA[<p>学习笔记：基本循环神经网络，对线性层的复用</p><h3 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a>问题引入</h3><ul><li>DNN：全链接神经网络，权重数量非常多</li><li>RNN：专门用于处理有时间序列关系的数据（如用之前的天气预测未来的天气），同样采用权重共享的概念（与CNN的内核相似），减少训练权重的数量，如天气、股市、自然语言</li><li>RNN Cell：本质是一个线性层，区别在于其是一个共享线性层，x1的输出值也参与x2输出值的计算，每次计算的线性层相同</li></ul><h3 id="RNN的计算过程"><a href="#RNN的计算过程" class="headerlink" title="RNN的计算过程"></a>RNN的计算过程</h3><ul><li><p>RNN Cell运算：$h_t &#x3D; tanh(W_{ih}x_ + b_{ih}+W_{hh}h_{h-1}+b_{hh})$</p></li><li><p>构造及实现：2种方式</p><ul><li>1.使用RNN Cell，自己构造循环</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">seq_len = <span class="number">3</span></span><br><span class="line">input_size = <span class="number">4</span></span><br><span class="line">hidden_size = <span class="number">2</span></span><br><span class="line">cell = torch.nn.RNNCell(input_size=input_size, hidden_size=hidden_size)</span><br><span class="line"><span class="comment"># seq_len放在最前，表示每一次从当前时刻取出一次数据</span></span><br><span class="line">dataset = torch.randn(seq_len, batch_size, input_size)</span><br><span class="line">hidden = torch.zeros(batch_size, hodden_size)</span><br><span class="line"><span class="keyword">for</span> idx, <span class="built_in">input</span> <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataset):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span>*<span class="number">20</span>, idx, <span class="string">&#x27;=&#x27;</span>*<span class="number">20</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Input size:&#x27;</span>, <span class="built_in">input</span>.shape)</span><br><span class="line">    hidden = cell(<span class="built_in">input</span>, hidden)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;outputs size:&#x27;</span>, hidden.shape)</span><br><span class="line">    <span class="built_in">print</span>(hidden)</span><br></pre></td></tr></table></figure><ul><li>2.直接使用RNN，自动循环</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">seq_len = <span class="number">3</span></span><br><span class="line">input_size = <span class="number">4</span></span><br><span class="line">hidden_size = <span class="number">2</span></span><br><span class="line">num_layers = <span class="number">1</span>  <span class="comment"># RNN的层数，RNN计算比较费时</span></span><br><span class="line">cell = torch.nn.RNNCell(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)</span><br><span class="line">inputs = torch.randn(seq_len, batch_size, input_size)</span><br><span class="line">hidden = torch.zeros(batch_size, hodden_size)</span><br><span class="line">out, hidden = cell(inputs, hidden)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Output size:&#x27;</span>,out.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Output:&#x27;</span>, out)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Hidden size:&#x27;</span>,hidden.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Hidden:&#x27;</span>, hidden)    </span><br></pre></td></tr></table></figure></li></ul><h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><ul><li><p>使用RNNCell处理文本：hello&gt;olelh</p></li><li><p>one-hot vector：构造词典，将文本变成独热向量</p></li><li><p>使用RNN后接一个人softMax，再计算交叉熵损失</p></li><li><p>代码实现</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">idx2char = [<span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;o&#x27;</span>]</span><br><span class="line">x_data = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">y_data = [<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">one_hot_lokup = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">x_one_hot = [one_hot_lookup[x] <span class="keyword">for</span> x <span class="keyword">in</span> x_data]</span><br><span class="line">inputs = torch.Tensor(x_one_hot).view(-<span class="number">1</span>, batch_size, input_size)</span><br><span class="line">labels = torch.LongTensor(y_data).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, batch_size</span>):</span><br><span class="line">        supper(Model, self).__init__()</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.rnncell = torch.nn.RNNCell(input_size=self.input_size, hidden_size=self.hidden_size)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden</span>):</span><br><span class="line">        hidden = self.rnncell(<span class="built_in">input</span>, hidden)</span><br><span class="line">        <span class="keyword">return</span> hidden</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">nint_hidden</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.zeros(self.batch_size, self.hidden_size)</span><br><span class="line">net = Model(input_size, hidden_size, batch_size)</span><br><span class="line">critersion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(net.paramenters(), lr=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># 使用训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">15</span>):</span><br><span class="line">    loss = <span class="number">0</span> </span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    hidden = net.init_hidden()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Predicted string: &#x27;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">input</span>, label <span class="keyword">in</span> zio(inputs, labels):</span><br><span class="line">        hidden = net(<span class="built_in">input</span>, hidden)</span><br><span class="line">        loss += criterison(hidden, label)</span><br><span class="line">        _, idx = hidden.<span class="built_in">max</span>(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="built_in">print</span>(idx2char[idx.item()], end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;, Epoch [%d/15] loss=%.4f&#x27;</span> % (epoch+<span class="number">1</span>, loss.item()))</span><br><span class="line"><span class="comment"># 使用RNN</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">15</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    outputs = net(inputs)</span><br><span class="line">    loss criterison(outputs, labels)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    _, idx = outputs.<span class="built_in">max</span>(dim=<span class="number">1</span>)</span><br><span class="line">    idx = idx.data.numpy()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Predicted: &#x27;</span>, <span class="string">&#x27;&#x27;</span>.join([idx2char[x] <span class="keyword">for</span> x <span class="keyword">in</span> idx]), end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;, Epoch [%d/15] loss = %.3f&#x27;</span>, % (epoch+<span class="number">1</span>, loss.item()))</span><br></pre></td></tr></table></figure></li><li><p>独热向量缺点：1.维度太高，2.向量过于稀疏，3.硬编码</p></li><li><p>EMBEDDING：将高维稀疏矩阵映射到稠密矩阵中，降维。实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        supper(Model, self).__init__()</span><br><span class="line">        self.emb = torch.nn.Embedding(input_size, embedding_size)</span><br><span class="line">        self.rnn = torch.nn.RNNCell(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(hidden_size, num_class)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        hidden = torch.zeros(num_layers, x_size(<span class="number">0</span>), hiddem_size)</span><br><span class="line">        x = self.emb(x)</span><br><span class="line">        x, _ = self.rnn(x, hidden)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x.view(-<span class="number">1</span>, num_calss)</span><br></pre></td></tr></table></figure></li></ul><h3 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h3><ul><li>尝试使用LSTM：多了一条路径，梯度传播时可以减小梯度为0的现象，效果比RNN好，但计算量大</li><li>尝试使用GRU：性能比RNN好，计算量比LSTM小</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>使用RNN步骤：</p><ul><li>了解序列Data的维度</li><li>理解循环过程中权重共享的机制</li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 刘二大人学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10-Advanced-CNN</title>
      <link href="/2022/09/26/10-Advanced-CNN/"/>
      <url>/2022/09/26/10-Advanced-CNN/</url>
      
        <content type="html"><![CDATA[<p>学习笔记：Advanced CNN</p><h3 id="复杂网络结构"><a href="#复杂网络结构" class="headerlink" title="复杂网络结构"></a>复杂网络结构</h3><ul><li><p>GoogleNet：包括Convolusion，Pooling，SoftMax，Other</p></li><li><p>Incepetion Model：</p><ul><li>1×1卷积：kernel_size &#x3D; 1，起到信息融合的作用，仅仅改变通道数量</li><li>1×1卷积作用：可以先缩小通道数，减少运算量，也称Network in Network</li><li>代码实现PPT中的Inception</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionA</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self in_channels</span>):</span><br><span class="line">        self.branchx1x = nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.branch5x5_1 = nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">self.branch5x5_2 = nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        self.branch3x3_1 = nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">self.branch3x3_2 = nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">self.branch3x3_3 = nn.Conv2d(<span class="number">24</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.branch_pool = nn.Conv2d(in_channels, <span class="number">24</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line">        </span><br><span class="line">        branch5x5 = self.branch5x5_1(x)</span><br><span class="line">branch5x5 = self.branch5x5_2(branch5x5)</span><br><span class="line">        </span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">branch3x3 = self.branch3x3_2(branch3x3)</span><br><span class="line">branch3x3 = self.branch3x3_3(branch3x3)</span><br><span class="line"></span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"><span class="comment"># 拼接</span></span><br><span class="line">outputs = [branch1x1, branch5x5, branch3x3, branch_pool]</span><br><span class="line"><span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">1</span>)  <span class="comment"># (batch_size, c, w, h)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">88</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.incep1 = InceptionA(in_channels=<span class="number">10</span>)</span><br><span class="line">        self.incep2 = InceptionA(in_channels=<span class="number">20</span>)</span><br><span class="line">        self.mp = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">1408</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self. x</span>):</span><br><span class="line">        in_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.mp(self.conv1(x)))</span><br><span class="line">        x = self.incep1(x)</span><br><span class="line">        x = F.relu(self.mp(self.conv2(x)))</span><br><span class="line">        x = self.incep2(x)</span><br><span class="line">        x = x.view(in_size, -<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></li></ul><h3 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h3><ul><li><p>梯度消失：在训练过程中，当计算梯度很小时，权重基本不会继续变化，反而会出现过拟合的情况。神经网络训练层数较多，为解决梯度消失采用ResNet</p></li><li><p>Residual Network：跳链接，$H(x) &#x3D; F(x)+x$</p></li><li><p>代码实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channels</span>):</span><br><span class="line">        spper(ResidualBlock, self).__init__()</span><br><span class="line">        self.channels = channels</span><br><span class="line">        self.conv1 = nn.Conv2d(channels, channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(channels, channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = F.relu(self.conv1(x))</span><br><span class="line">        y = self.conv2(y)</span><br><span class="line">        <span class="keyword">return</span> F.relu(x+y)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        supper(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernal_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernal_size=<span class="number">5</span>)</span><br><span class="line">        self.mp = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.rblock1 = ResidualBlock(<span class="number">16</span>)</span><br><span class="line">        self.rblock2 = ResidualBlock(<span class="number">32</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        in_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = self.mp(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.rblock1(x)</span><br><span class="line">        x = self.mp(F.relu(self.conv2(x)))</span><br><span class="line">        x = self.rblock2(x)</span><br><span class="line">        x = x.view(in_size, -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></li></ul><h3 id="课后练习"><a href="#课后练习" class="headerlink" title="课后练习"></a>课后练习</h3><ul><li>实现几种论文中的Residual Block：*Identity Mapping in Deep Residual Networks[C]*，用MNIST做测试</li><li>实现DenseNet：<em>Densely Connected Convolutional Networks[J]</em></li></ul><h3 id="后续学习"><a href="#后续学习" class="headerlink" title="后续学习"></a>后续学习</h3><ul><li>前述内容偏重于实现，后续应该从理论的角度深入理解深度学习模型</li><li>理论：《深度学习》</li><li>阅读Pytorch的文档，API和Reference</li><li>复现一些经典工作（论文中），先阅读代码（4个步骤），然后尝试自己来写</li><li>选择特定领域，阅读相关论文，扩充自己的视野</li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 刘二大人学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cornell大学对博士的要求</title>
      <link href="/2022/09/24/Cornell%E5%A4%A7%E5%AD%A6%E5%AF%B9%E5%8D%9A%E5%A3%AB%E7%9A%84%E8%A6%81%E6%B1%82/"/>
      <url>/2022/09/24/Cornell%E5%A4%A7%E5%AD%A6%E5%AF%B9%E5%8D%9A%E5%A3%AB%E7%9A%84%E8%A6%81%E6%B1%82/</url>
      
        <content type="html"><![CDATA[<hr><p>​在平时的学习和科研工做中，自己会不断思考博士期间研究工作如何进行，要进行到什么程度以及毕业了要达到什么标准等等。常说博士的研究往往是人类认知的边界，导师大多是在方向上进行指导。我非常认可这些说法，到博士阶段需要自己有很强的主观能动性去推进自己的毕业课题。博士生涯已经过去一年，回想起来基本可以说是荒废了，没有确定好课题，也没有为毕业方向的研究打好基础。被一些<em>狗屁</em>项目缠身并不是借口，还是没有以一个高标准要求自己，自己也不够主动去推进。</p><p>​最近读到了康奈尔大学对博士生的要求，我个人非常认可，这里记录到Blog里，希望与有缘看到的你分享共勉。现在开始努力，相信一定不晚！</p><p>​<code>种一棵树最好的时间是十年之前，其次就是现在</code>—————————————————-<em>C1005 xxx</em></p><h1 id="康奈尔大学对博士生的要求"><a href="#康奈尔大学对博士生的要求" class="headerlink" title="康奈尔大学对博士生的要求"></a>康奈尔大学对博士生的要求</h1><p><em>A candidate for a doctoral degree is expected to demonstrate mastery of knowledge in the chosen discipline and to synthesize and create new knowledge, making an original and substantial contribution to the discipline in an appropriate time frame.</em></p><p>博士学位候选人应该表现出能掌握所选学科的知识，并能综合和创造新知识，在适当的时间范围内对该学科做出创造性的实质性贡献。</p><h2 id="1-为学科做出原创性的实质性贡献"><a href="#1-为学科做出原创性的实质性贡献" class="headerlink" title="1. 为学科做出原创性的实质性贡献"></a>1. 为学科做出原创性的实质性贡献</h2><p><em><strong>Make an original and substantial contribution to the discipline</strong></em></p><p><em>Think originally and independently to develop concepts and methodologies; Identify new research opportunities within one’s field.</em></p><p>通过独立地和独创性地思考来发展思想和方法；识别自己领域内的新研究机会。</p><h2 id="2-展示高级别的研究技能"><a href="#2-展示高级别的研究技能" class="headerlink" title="2. 展示高级别的研究技能"></a>2. 展示高级别的研究技能</h2><p><em><strong>Demonstrate advanced research skill</strong></em></p><p><em>Synthesize existing knowledge, identifying and accessing appropriate resources and other sources of relevant information and critically analyzing and evaluating one’s own findings and those of others; Master application of existing research methodologies, techniques, and technical skills; Communicate in a style appropriate to the discipline.</em></p><p>综合现有知识，确定和获取适当的资源和其他相关信息来源，批判性地分析和评估自己和他人的发现；掌握现有研究方法、技术和技能的应用；以适合本学科的方式进行交流。</p><h2 id="3-展示对提升学术研究价值的承诺"><a href="#3-展示对提升学术研究价值的承诺" class="headerlink" title="3. 展示对提升学术研究价值的承诺"></a>3. 展示对提升学术研究价值的承诺</h2><p><em><strong>Demonstrate commitment to advancing the values of scholarship</strong></em></p><p><em>Keep abreast of current advances within one’s field and related areas; Show commitment to personal professional development through engagement in professional societies, publication, and other knowledge transfer modes；Show a commitment to creating an environment that supports learning through teaching, collaborative inquiry, mentoring, or demonstration.</em></p><p>及时了解自己领域和相关领域的最新进展；通过加入专业团体、发表论文和参与其他知识转移模式来展示对个人专业发展的承诺；承诺通过教学、合作探究、辅导他人或示范来创造一个支持学习的环境。</p><h2 id="4-展示专业技能"><a href="#4-展示专业技能" class="headerlink" title="4. 展示专业技能"></a>4. 展示专业技能</h2><p><em><strong>Demonstrate professional skills</strong></em></p><p><em>Adhere to ethical standards in the discipline；Listen, give, and receive feedback effectively.</em></p><p>遵守学科中的道德标准；有效地倾听、给与和接受反馈意见。</p>]]></content>
      
      
      <categories>
          
          <category> 科研总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 心得体会 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>09-Basic-CNN</title>
      <link href="/2022/09/24/09-Basic-CNN/"/>
      <url>/2022/09/24/09-Basic-CNN/</url>
      
        <content type="html"><![CDATA[<p>学习笔记：Convolutional Neural Networks，卷积神经网络基础</p><h3 id="初步认识"><a href="#初步认识" class="headerlink" title="初步认识"></a>初步认识</h3><ul><li>全链接神经网络：网络中全用的是线性层，输入值和每一个输出值之间都存在权重，每一个输入节点都要参与到下一层的计算中</li><li>二维卷积神经网络：全链接神经网络会丧失空间的信息，需要做卷积运算<ul><li>工作方式：input: 1*28*28(C*W*H)，Convolution: 可以改变通道和维度，Subsampling：通道数不变，宽度和高度会改变，目的是减少数据量，降低运算需求。</li><li>最后运用全链接层，运用交叉熵损失，映射到N维的输出。需要明确输入和输出的维度。</li><li>特征提取：Convolution， Subsampling</li></ul></li></ul><h3 id="卷积介绍"><a href="#卷积介绍" class="headerlink" title="卷积介绍"></a>卷积介绍</h3><ul><li><p>图像：栅格图像（直接捕获，放大过后会变模糊）、矢量图像（程序生成如viso，放大不会失真），RGB颜色模式</p></li><li><p>单通道卷积：用卷积内核遍历，对应元素相乘得到结果</p></li><li><p>3通道卷积：每一个通道配一个卷积核，对每个通道遍历后，对应元素相加得到结果</p></li><li><p>N通道卷积：每个卷积核的通道数n必须与N相同，需要输出m个通道数的话就得准备m个卷积核（每个核的维度必须相等），再按照顺序拼接起来。</p></li><li><p>演示计算过程：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">in_channels, out_channels = <span class="number">5</span>, <span class="number">10</span></span><br><span class="line">width, height = <span class="number">100</span>, <span class="number">100</span></span><br><span class="line">kernel_size = <span class="number">3</span><span class="comment"># 卷积核的大小</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line"><span class="comment"># 输入batch, C * W * H</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(batch_size, in_channels, width, height)</span><br><span class="line"><span class="comment"># 至少输入3个参数，输入通道数，输出通道数，卷积核大小 </span></span><br><span class="line">conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size)</span><br><span class="line">output = conv_layer(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"><span class="built_in">print</span>(conv_layer.weght.shape)</span><br></pre></td></tr></table></figure></li><li><p>padding：使输出的图像大小不变，在外侧填充。常用填充0</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">input</span> = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span></span><br><span class="line">         <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span></span><br><span class="line">         <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">9</span></span><br><span class="line">         <span class="number">9</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">5</span></span><br><span class="line">         <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">1</span>]</span><br><span class="line"><span class="built_in">input</span> = torch.Tensor(<span class="built_in">input</span>).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">conv_layer = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bais=<span class="literal">False</span>)</span><br><span class="line">kernel = torch.Tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">conv_layer.weight.data = kernel.data</span><br><span class="line">output = conv_layer(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure></li><li><p>stride&#x3D;n：遍历过程中，可以跳过n个单元格</p></li><li><p>Max Pooling Layer(池化层)：默认stride&#x3D;2, 通道数量不变，图像大小减半</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">input</span> = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span></span><br><span class="line">         <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span></span><br><span class="line">         <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">9</span></span><br><span class="line">         <span class="number">9</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">5</span></span><br><span class="line">         <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">1</span>]</span><br><span class="line"><span class="built_in">input</span> = torch.Tensor(<span class="built_in">input</span>).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">maxpooling_layer = torch.nn.MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line">output = maxpooling_layer(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure></li></ul><h3 id="卷积神经网络实现MNIST"><a href="#卷积神经网络实现MNIST" class="headerlink" title="卷积神经网络实现MNIST"></a>卷积神经网络实现MNIST</h3><ul><li><p>使用GPU的步骤</p><ul><li><p>将模型迁移至GPU上，将训练数据也迁移至GPU，模型核数据需要在同一块显卡上，使用以下语句</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span><span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">model.to(device)</span><br><span class="line">inputs, target = inputs.to(device), target.to(device)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>代码实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.1307</span>, ), (<span class="number">0.3081</span>, ))</span><br><span class="line">])</span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=batch_size)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_loader = DataLoader(test_dataset, shuffle=<span class="literal">False</span>, batch_size=batch_size)</span><br><span class="line"><span class="comment"># Design model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pooling = torch.nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(<span class="number">320</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.pooling(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.pooling(self.conv2(x)))</span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>)  <span class="comment"># -1表示自动计算该处的值</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">model = Net()</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="comment"># Construct loss and optimizer</span></span><br><span class="line">criterison = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># Training Cycle</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    runing_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, target = data</span><br><span class="line">        inputs, target = inputs.to(device), target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterison(outputs, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        runing_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.5f&#x27;</span> % (epoch+<span class="number">1</span>, batch_idx+<span class="number">1</span>, runing_loss/<span class="number">300</span>))</span><br><span class="line">            runing_loss = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span>          </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            images, labels = images.to(device), labels.to(device)</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            <span class="comment"># dim=1表示沿着第一个维度（行）去寻找最大值的下标</span></span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 预测结果为N*1的矩阵，size(0)表示返回N</span></span><br><span class="line">            total +=labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy on test set: %d %%&#x27;</span> % (<span class="number">100</span> * correct /total))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        test()</span><br></pre></td></tr></table></figure></li></ul><h3 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h3><ul><li>尝试更复杂的CNN：卷积层3个，ReLU3个，池化层3个，线性层3个。</li><li>做不同配置下的性能比较</li><li>代码实现：</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.conv3 = torch.nn.Conv2d(<span class="number">20</span>, <span class="number">40</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pooling = torch.nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc1 = torch.nn.Linear(<span class="number">360</span>, <span class="number">180</span>)</span><br><span class="line">        self.fc2 = torch.nn.Linear(<span class="number">180</span>, <span class="number">100</span>)</span><br><span class="line">        self.fc3 = torch.nn.Linear(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.pooling(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.pooling(self.conv2(x)))</span><br><span class="line">        x = F.relu(self.pooling(self.conv3(x)))</span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>)  <span class="comment"># -1表示自动计算该处的值</span></span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">model = Net()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 刘二大人学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>08-Softmax-Classifier</title>
      <link href="/2022/09/21/08-Softmax-Classifier/"/>
      <url>/2022/09/21/08-Softmax-Classifier/</url>
      
        <content type="html"><![CDATA[<p>学习笔记：Softmax分类器如何解决多分类问题以及用PyTorch如何实现</p><h3 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h3><ul><li>输出：输出的概率满足一个分布（和为1）</li><li>操作：前面的神经层仍然使用Sigmoid，最后一层使用Softmax</li><li>Softmax：1. 如何把线性计算记过变成全正值（进行指数运算），2.如何让他们的和为1（进行归一化处理）</li><li>计算公式：$P(y&#x3D;i) &#x3D; \frac{e^{Z_i}}{\Sigma_{j&#x3D;0}^{K-1}e^{Z_j}}$，$Z_i$为上一线性层的输出</li></ul><h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h3><ul><li><p>多分类问题中交叉熵计算公式：$\H(P, Q) &#x3D; -\Sigma_{i&#x3D;1}^n \Sigma_{j&#x3D;1}^m P(X_{ij})\log(Q(X_{ij}))$</p><p>其中，m为类别数，n为样本数，P(Xij)当i&#x3D;j时取1，不等时取0，Q(Xij)为样本i的预测值为j的概率</p></li><li><p>简化：由于P(Xij)非0即1，且计算过程中只有一个1，因此一个样本的loss值计算可以简化为：</p><p>$Loss_i &#x3D; -\Sigma_{j&#x3D;1}^m P(X_{ii}) \log(Q(X_{ij})) &#x3D; -\Sigma_{j&#x3D;1}^m\log(Q(X_{ij})) &#x3D; -Y\log\ \hat{Y}$</p><p>此处Y是作为独热编码（one-hot）输入的，对离散变量进行分类，相等值时为1，其他值时为0</p></li><li><p>课后作业：CrossEntropyLoss与NLLLoss之间有什么区别以及计算关系？</p><ul><li><a href="https://pytorch.org/docs/stable/nn.html#nllloss">https://pytorch.org/docs/stable/nn.html#nllloss</a></li><li><a href="https://pytorch.org/docs/stable/nn.html#crossentropyloss">https://pytorch.org/docs/stable/nn.html#crossentropyloss</a></li></ul></li></ul><h3 id="用Softmax和CrossEntropyLoss处理MNIST数据集"><a href="#用Softmax和CrossEntropyLoss处理MNIST数据集" class="headerlink" title="用Softmax和CrossEntropyLoss处理MNIST数据集"></a>用Softmax和CrossEntropyLoss处理MNIST数据集</h3><ul><li><p>输入X：28*28的矩阵，做线性映射至0~1之间</p></li><li><p>步骤：Prepare Dataset, Design Model, Construct Loss and optimizer, Training cycle and test</p></li><li><p>代码实现</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment">#将调用Relu</span></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms      <span class="comment">#用于图像处理的工具包</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets        <span class="comment">#导入</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment"># 神经网络希望输入值比较小，最好[-1， 1]，最好符合正态分布</span></span><br><span class="line"><span class="comment"># Py读取图像时用的是PIL，PIL读进来的图像是28*28，象素值是&#123;0，255&#125;</span></span><br><span class="line"><span class="comment"># 需要转换成1*28*28的图像张量，象素值&#123;0，1&#125;</span></span><br><span class="line"><span class="comment"># 灰色图像是单通道，彩色图像是多通道RGB</span></span><br><span class="line"><span class="comment"># ToTensor是将原始图像变成1*28*28的张量</span></span><br><span class="line"><span class="comment"># Normalize是正则化，第一个参数是均值，第二个是标准差，使其满足0~1分布</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.1307</span>, ), (<span class="number">0.3081</span>, ))</span><br><span class="line">])</span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=batch_size)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_loader = DataLoader(test_dataset, shuffle=<span class="literal">False</span>, batch_size=batch_size)</span><br><span class="line"><span class="comment"># Design model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.l1 = torch.nn.Linear(<span class="number">784</span>, <span class="number">512</span>)</span><br><span class="line">        self.l2 = torch.nn.Linear(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">        self.l3 = torch.nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.l4 = torch.nn.Linear(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.l5 = torch.nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 将输入张量变成二阶张量，784列，-1用来填充batch_size</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">784</span>)</span><br><span class="line">        x = F.relu(self.l1(x))</span><br><span class="line">        x = F.relu(self.l2(x))</span><br><span class="line">        x = F.relu(self.l3(x))</span><br><span class="line">        x = F.relu(self.l4(x))</span><br><span class="line">        <span class="keyword">return</span> self.l5(x)</span><br><span class="line">model = Net()</span><br><span class="line"><span class="comment"># Construct loss and optimizer</span></span><br><span class="line">criterison = torch.nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#随机梯度下降，momentum表冲量，在更新时一定程度上保留原方向</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># Training Cycle</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    runing_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, target = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterison(outputs, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        runing_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch+<span class="number">1</span>, batch_idx+<span class="number">1</span>, runing_loss/<span class="number">300</span>))</span><br><span class="line">            runing_loss = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    correct = <span class="number">0</span>     <span class="comment">#正确数</span></span><br><span class="line">    total = <span class="number">0</span>          <span class="comment">#总数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            <span class="comment"># dim=1表示沿着第一个维度（行）去寻找最大值的下标</span></span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 预测结果为N*1的矩阵，size(0)表示返回N</span></span><br><span class="line">            total +=labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy on test set: %d %%&#x27;</span> % (<span class="number">100</span> * correct /total))</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        test()</span><br></pre></td></tr></table></figure></li></ul><h3 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h3><ul><li><p>对kaggle上的otto数据集进行多分类预测，</p></li><li><p>代码实现</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line"><span class="comment"># Prepare Datase</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">labelsToid</span>(<span class="params">labels</span>):</span><br><span class="line">    ids = []</span><br><span class="line">    labels_name = [<span class="string">&#x27;Class_1&#x27;</span>, <span class="string">&#x27;Class_2&#x27;</span>, <span class="string">&#x27;Class_3&#x27;</span>, <span class="string">&#x27;Class_4&#x27;</span>, <span class="string">&#x27;Class_5&#x27;</span>, <span class="string">&#x27;Class_6&#x27;</span>, <span class="string">&#x27;Class_7&#x27;</span>, <span class="string">&#x27;Class_8&#x27;</span>, <span class="string">&#x27;Class_9&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">        ids.append(labels_name.index(label))</span><br><span class="line">    <span class="keyword">return</span> ids</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OttoDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filepath</span>):</span><br><span class="line">        xy = pd.read_csv(filepath)</span><br><span class="line">        self.<span class="built_in">len</span> = xy.shape[<span class="number">0</span>]</span><br><span class="line">        self.x_data = torch.from_numpy(np.array(xy)[:, <span class="number">1</span>:-<span class="number">1</span>].astype(<span class="built_in">float</span>))</span><br><span class="line">        y = labelsToid(xy[<span class="string">&#x27;target&#x27;</span>])</span><br><span class="line">        self.y_data = y</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.x_data[index], self.y_data[index]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line">train_data = OttoDataset(<span class="string">&#x27;otto/train.csv&#x27;</span>)</span><br><span class="line">train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">&#x27;otto/test.csv&#x27;</span>)</span><br><span class="line">test_inputs = torch.tensor(np.array(test_data)[:, <span class="number">1</span>:].astype(<span class="built_in">float</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Design model using pytorch API</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.l1 = torch.nn.Linear(<span class="number">93</span>, <span class="number">64</span>)</span><br><span class="line">        self.l2 = torch.nn.Linear(<span class="number">64</span>, <span class="number">32</span>)</span><br><span class="line">        self.l3 = torch.nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line">        self.l4 = torch.nn.Linear(<span class="number">16</span>, <span class="number">9</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.l1(x))</span><br><span class="line">        x = F.relu(self.l2(x))</span><br><span class="line">        x = F.relu(self.l3(x))</span><br><span class="line">        <span class="keyword">return</span> self.l4(x)</span><br><span class="line">model = Net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct loss and optimizer</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training cycle</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    runing_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, labels = data</span><br><span class="line">        inputs = inputs.<span class="built_in">float</span>()</span><br><span class="line">        <span class="comment">#labels = labels.float()</span></span><br><span class="line">        y_pred = model(inputs)</span><br><span class="line">        loss = criterion(y_pred, labels)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        runing_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch+<span class="number">1</span>, i+<span class="number">1</span>, runing_loss/<span class="number">300</span>))</span><br><span class="line">            runing_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>():</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        labels_pred = model(test_inputs.<span class="built_in">float</span>())</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(labels_pred, dim=<span class="number">1</span>)</span><br><span class="line">        reslut = pd.get_dummies(predicted)</span><br><span class="line">        labels_name = [<span class="string">&#x27;Class_1&#x27;</span>, <span class="string">&#x27;Class_2&#x27;</span>, <span class="string">&#x27;Class_3&#x27;</span>, <span class="string">&#x27;Class_4&#x27;</span>, <span class="string">&#x27;Class_5&#x27;</span>, <span class="string">&#x27;Class_6&#x27;</span>, <span class="string">&#x27;Class_7&#x27;</span>, <span class="string">&#x27;Class_8&#x27;</span>, <span class="string">&#x27;Class_9&#x27;</span>]</span><br><span class="line">        reslut.columns = labels_name</span><br><span class="line">        reslut.insert(<span class="number">0</span>, <span class="string">&#x27;id&#x27;</span>, test_data[<span class="string">&#x27;id&#x27;</span>])</span><br><span class="line">        output = pd.DataFrame(reslut)</span><br><span class="line">        output.to_csv(<span class="string">&#x27;Otto_predict.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        predict()</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 刘二大人学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>07-Dataset-DataLoader</title>
      <link href="/2022/09/17/07-Dataset-DataLoader/"/>
      <url>/2022/09/17/07-Dataset-DataLoader/</url>
      
        <content type="html"><![CDATA[<p>学习笔记：学习加载数据的两个工具类Dataset 和DataLoader</p><ul><li>Dataset：构造数据集，支持索引（使用下标方式索引）</li><li>DataLoader：拿出mini-Batch使用</li><li>理解Mini-Batch的batch size大小对训练时间的影响？</li></ul><h3 id="Dataset和DataLoader的用法"><a href="#Dataset和DataLoader的用法" class="headerlink" title="Dataset和DataLoader的用法"></a>Dataset和DataLoader的用法</h3><ul><li><p><strong>问题引出</strong>：GD——使用全部的Batch，可以最大化利用向量计算的优势，提高计算的速度，存在鞍点，性能可能会遇到问题；SGD——只使用一个样本，得到较好的随机性，可以克服鞍点问题，性能好，无法利用CPU和GPU的并行能力，训练时间非常长。</p></li><li><p><strong>Mini-Batch</strong>：均衡性能和训练时间上的需求。三个重要概念：<em>Epoch, Batch-Size, Iterations</em></p><ul><li>epoch：所有样本都参与了完整训练（前馈和反馈）的过程</li><li>Batch-Size：每次完整训练时所用的样本数量（一个mini-Batch所含样本数）</li><li>Iteration：一个分了多少个Batch（需要执行多少次内循环），也就是mini-Batch的数量</li></ul></li><li><p>使用方法：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Training cycle</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(trainig_epochs):</span><br><span class="line">    <span class="comment"># Loop over all batches</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(total_batch):</span><br></pre></td></tr></table></figure></li><li><p><strong>DataLoader</strong>：小批量计算所需指定的重要参数——batch_size&#x3D;2，shuffle&#x3D;True(打乱，提高随机性)</p></li><li><p>代码实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset<span class="comment">#抽象类，不能实例化，只能被继承，通过继承来构造自己的Dataset类</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader<span class="comment"># 可实例化</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DiabetesDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="comment"># init有两种方法</span></span><br><span class="line">    <span class="comment"># 1. All Data，适合于数据量不大</span></span><br><span class="line">    <span class="comment"># 2. 把文件名放进列表里，需要时现去读取对应文件，适用于数据量大</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):<span class="comment">#实例化过后支持下标索引，dataset[index]</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):<span class="comment"># 可以返回数据个数</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">detaset = DiabetesDataset()<span class="comment">#实例化</span></span><br><span class="line"><span class="comment"># DataLoader是 pytorch提供的加载器，初始化时至少需要提供4个参数</span></span><br><span class="line">train_loader = DataLoader(dataset=dataset,<span class="comment"># 传递数据集</span></span><br><span class="line">                         batch_size=<span class="number">32</span>,<span class="comment"># 一个mini-batch的容量</span></span><br><span class="line">                         shuffle=<span class="literal">True</span>,<span class="comment"># 是否打乱，一般打乱</span></span><br><span class="line">                         num_works=<span class="number">2</span>)<span class="comment"># 读数据的时候用n个进程</span></span><br><span class="line">...</span><br><span class="line">if_name_==<span class="string">&#x27;_main_&#x27;</span>:<span class="comment"># 在Win下缺少这步时会出现问题，原因：Linx（使用fork）和Win（使用spawn)的多进程调用不一样，会导致运行错误。需要将要运行代码封装起来，如这个语句</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">            <span class="comment"># 1. Prepare data</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="课堂练习"><a href="#课堂练习" class="headerlink" title="课堂练习"></a>课堂练习</h3><ul><li><p>问题：使用mini-batch构建糖尿病神经网络训练模型</p></li><li><p>代码实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="comment"># 构造自己的Dataset类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DiabetesDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filepath</span>):</span><br><span class="line">        xy = np.loadtxt(filepath, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=np.float32)</span><br><span class="line">        self.<span class="built_in">len</span> = xy.shape[<span class="number">0</span>]</span><br><span class="line">        self.x_data = torch.from_numpy(xy[:, :-<span class="number">1</span>])</span><br><span class="line">        self.y_data = torch.from_numpy(xy[:, [-<span class="number">1</span>]])</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.x_data[index], self.y_data[index]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">dataset = DiabetesDataset(<span class="string">&#x27;diabetes.csv.gz&#x27;</span>)</span><br><span class="line">train_loader = DataLoader(dataset=dataset,</span><br><span class="line">                          batch_size=<span class="number">32</span>,</span><br><span class="line">                          shuffle=<span class="literal">True</span>,</span><br><span class="line">                          num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 使用类构建模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        self.sigmoid = torch.nn.Sigmoid()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.sigmoid(self.linear1(x))</span><br><span class="line">        x = self.sigmoid(self.linear2(x))</span><br><span class="line">        x = self.sigmoid(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">model = Model()</span><br><span class="line"><span class="comment"># 构建损失函数和优化器</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># Training cycle</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">            inputs, labels = data</span><br><span class="line">            y_pred = model(inputs)</span><br><span class="line">            loss = criterion(y_pred, labels)</span><br><span class="line">            <span class="built_in">print</span>(epoch, i, loss.item())</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br></pre></td></tr></table></figure></li><li><p>torchvision中提供的常用数据集有：MNIST, Fashion-MNIST, EMNIST, COCO, LSUN, ImageFlder, DatasetFloder, iMAGENET-12, CIFAR, STL10, PhotoTour等，都是torch.utils.data.Dataset的子类，都可以按照上述方式加载</p></li><li><p>使用实例：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># example 1</span></span><br><span class="line">imagenet_data = torchvision.datasets.ImageFolder(<span class="string">&#x27;path/to/imagenet_root&#x27;</span>)</span><br><span class="line">data_loader = torch.uils.data.DataLoader(imagenet_data,batch_size=<span class="number">4</span>,shuffle=<span class="literal">True</span>,nnum_workers=args.nThreads)</span><br><span class="line"></span><br><span class="line"><span class="comment"># example 2</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> tranforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets<span class="comment"># 使torchvision数据集时需要导入这个库</span></span><br><span class="line">train_dataset = dataset.MNIST(root=<span class="string">&#x27;../dataset/mnist&#x27;</span>, train=<span class="literal">True</span>, transform=transform.ToTensor(), download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = dataset.MNIST(root=<span class="string">&#x27;../dataset/mnist&#x27;</span>, train=<span class="literal">False</span>, transform=transform.ToTensor(), download=<span class="literal">True</span>)</span><br><span class="line">train_loader = DataLoader(dataset=train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(dataset=test_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h3><ul><li><p><strong>kaggle</strong>：数据科学的竞赛网站，包括机器学习的训练数据等，入门级数据集：titanic</p></li><li><p><strong>作业</strong>：使用DataLoader构造分类器，对数据进行分类，判断一个乘客是否存活</p></li><li><p>网站：<a href="https://www.kaggle.com/c/titanic/data">https://www.kaggle.com/c/titanic/data</a></p></li><li><p>代码实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="comment"># Prepare Dataset</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TitanicDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filepath</span>):</span><br><span class="line">        train_data = pd.read_csv(filepath)</span><br><span class="line">        <span class="comment">#train_data[&#x27;Age&#x27;] = train_data[&#x27;Age&#x27;].fillna(train_data[&#x27;Age&#x27;].mean())         # Age中存在空值</span></span><br><span class="line">        featrues = [<span class="string">&quot;Pclass&quot;</span>, <span class="string">&quot;Sex&quot;</span>, <span class="string">&quot;SibSp&quot;</span>, <span class="string">&quot;Parch&quot;</span>, <span class="string">&quot;Fare&quot;</span>, <span class="string">&quot;Embarked&quot;</span>]</span><br><span class="line">        self.x_data = torch.from_numpy(np.array(pd.get_dummies(train_data[featrues])))</span><br><span class="line">        self.y_data = torch.from_numpy(np.array(train_data[<span class="string">&quot;Survived&quot;</span>]))</span><br><span class="line">        self.<span class="built_in">len</span> = train_data.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.x_data[index], self.y_data[index]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line">dataset = TitanicDataset(<span class="string">&#x27;titanic/train.csv&#x27;</span>)</span><br><span class="line">train_loader = DataLoader(dataset=dataset, batch_size=<span class="number">10</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># Design model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">9</span>, <span class="number">5</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.sigmoid = torch.nn.Sigmoid()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.sigmoid(self.linear1(x))</span><br><span class="line">        x = self.sigmoid(self.linear2(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predic</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            x = self.sigmoid(self.linear1(x))</span><br><span class="line">            x = self.sigmoid(self.linear2(x))</span><br><span class="line">            y = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> x:</span><br><span class="line">                <span class="keyword">if</span> i &gt; <span class="number">0.5</span>:</span><br><span class="line">                    y.append(<span class="number">1</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    y.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> y</span><br><span class="line">model = Model()</span><br><span class="line"><span class="comment"># Consruct loss and optimizer</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.005</span>)</span><br><span class="line"><span class="comment"># Training cycle</span></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loss_list = []</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">            inputs, labels = data</span><br><span class="line">            inputs = inputs.<span class="built_in">float</span>()</span><br><span class="line">            labels = labels.<span class="built_in">float</span>()</span><br><span class="line">            y_pred = model(inputs)</span><br><span class="line">            y_pred =y_pred.squeeze(-<span class="number">1</span>)</span><br><span class="line">            loss = criterion(y_pred, labels)</span><br><span class="line">            <span class="built_in">print</span>(epoch, i, loss.item())</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        loss_list.append(loss.item())</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="number">100</span>), loss_list)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="comment"># Prediction</span></span><br><span class="line">test_data = pd.read_csv(<span class="string">&#x27;titanic/test.csv&#x27;</span>)</span><br><span class="line"><span class="comment">#test_data[&#x27;Age&#x27;] = test_data[&#x27;Age&#x27;].fillna(test_data[&#x27;Age&#x27;].mean())</span></span><br><span class="line">featrue_x = [<span class="string">&quot;Pclass&quot;</span>, <span class="string">&quot;Sex&quot;</span>, <span class="string">&quot;SibSp&quot;</span>, <span class="string">&quot;Parch&quot;</span>, <span class="string">&quot;Fare&quot;</span>, <span class="string">&quot;Embarked&quot;</span>]</span><br><span class="line">x_test = torch.from_numpy(np.array(pd.get_dummies(test_data[featrue_x])))</span><br><span class="line">y_test = model.predic(x_test.<span class="built_in">float</span>())</span><br><span class="line">outputs = pd.DataFrame(&#123;<span class="string">&#x27;PassengerId&#x27;</span>: test_data.PassengerId, <span class="string">&#x27;Survived&#x27;</span>: y_test&#125;)</span><br><span class="line">outputs.to_csv(<span class="string">&#x27;predict_titanic.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">outputs.head()</span><br></pre></td></tr></table></figure><p>按照网站要求格式，提交以上模型预测结果，得分：0.72248</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 刘二大人学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>06-Multiple-Dimension-Input</title>
      <link href="/2022/09/17/06-Multiple-Dimension-Input/"/>
      <url>/2022/09/17/06-Multiple-Dimension-Input/</url>
      
        <content type="html"><![CDATA[<p>学习笔记：多维输入问题</p><h3 id="问题提出"><a href="#问题提出" class="headerlink" title="问题提出"></a>问题提出</h3><ul><li><p>糖尿病数据（Diabetes)：每一行叫做一个样本 sample，每一列叫做特征 Feature</p></li><li><p>多维逻辑回归模型：$\hat{y}^{(i)} &#x3D; \sigma(\Sigma_n^8 x^{(i)}_n \omega+b)$，i表示某一个样本，n表示维度</p></li><li><p>模型构建时所需做的修改：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.linear = torch.nn.Linear(n, m)<span class="comment">#n维输入，m维输出</span></span><br><span class="line"><span class="comment"># 可以降维也可以升维，变换的维度和层数决定了网络的复杂度</span></span><br><span class="line"><span class="comment"># 越复杂学习能力学强，学习能力过强容易学习到噪声，不具备泛化能力。</span></span><br><span class="line"><span class="comment"># 具体多少层/每层多少个神经元合适，需要超参数搜索方法进行尝试</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="课堂练习"><a href="#课堂练习" class="headerlink" title="课堂练习"></a>课堂练习</h3><ul><li><p>问题：糖尿病预测，数据 diabetes.csv.gz</p></li><li><p>步骤：1. Prepare Dataset2. Design model using Class3. Construct loss and optimizer4. Training cycle</p></li><li><p>代码实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare Dataset</span></span><br><span class="line">xy = np.loadtxt(<span class="string">&#x27;diabetes.csv.gz&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=np.float32)</span><br><span class="line">x_data = torch.from_numpy(xy[:, :-<span class="number">1</span>])   <span class="comment">#第一列开始，最后一列不要</span></span><br><span class="line">y_data = torch.from_numpy(xy[:, [-<span class="number">1</span>]])  <span class="comment"># [-1]表示取出的是一个矩阵</span></span><br><span class="line"><span class="comment">#Design model using Class</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        self.sigmod = torch.nn.Sigmoid()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.sigmod(self.linear1(x))</span><br><span class="line">        x = self.sigmod(self.linear2(x))</span><br><span class="line">        x = self.sigmod(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">model = Model()</span><br><span class="line"><span class="comment"># Constructe loss and optimizer</span></span><br><span class="line">criterion = torch.nn.BCELoss(size_average=<span class="literal">True</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># Training Cycle</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 全扔进去，不是使用小批量，后续会讲</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure></li></ul><h3 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h3><ul><li><p>问题：尝试不同的激活函数（active function）</p></li><li><p>代码实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare Dataset</span></span><br><span class="line">xy = np.loadtxt(<span class="string">&#x27;diabetes.csv.gz&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=np.float32)</span><br><span class="line">x_data = torch.from_numpy(xy[:, :-<span class="number">1</span>])   <span class="comment">#第一列开始，最后一列不要</span></span><br><span class="line">y_data = torch.from_numpy(xy[:, [-<span class="number">1</span>]])  <span class="comment"># [-1]表示取出的是一个矩阵</span></span><br><span class="line"><span class="comment">#Design model using Class</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        self.sigmod = torch.nn.Sigmoid()</span><br><span class="line">        self.active = torch.nn.ReLU()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#x = self.sigmod(self.linear1(x))</span></span><br><span class="line">        x = self.active(self.linear1(x))</span><br><span class="line">        <span class="comment">#x = self.sigmod(self.linear2(x))</span></span><br><span class="line">        x = self.active(self.linear2(x))</span><br><span class="line">        x = self.sigmod(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">model = Model()</span><br><span class="line"><span class="comment"># Constructe loss and optimizer</span></span><br><span class="line">criterion = torch.nn.BCELoss(size_average=<span class="literal">True</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># Training Cycle</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 全扔进去，不是使用小批量，后续会讲</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 刘二大人学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>05-Logistic-Regression</title>
      <link href="/2022/09/15/05-Logistic-Regression/"/>
      <url>/2022/09/15/05-Logistic-Regression/</url>
      
        <content type="html"><![CDATA[<p>学习笔记：理解逻辑回归，二分类问题</p><h3 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h3><ul><li><p>回归任务：要估计值为连续值；分类问题：估算结果为离散值，属于某个集合。</p></li><li><p>常用数据集：(1) MINST，training set: 60000个，test set: 10000个，class: 10</p><p>​(2) CIFAR-10，training set: 50000个，test set: 10000个，class: 10</p></li><li><p>使用方法：工具包torchvision，提供相应的数据集，</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvison</span><br><span class="line"><span class="comment"># 使用MNIST</span></span><br><span class="line">train_set = torchvision.datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">test_set = torchvision.datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 使用CIFA-10</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(...)</span><br><span class="line">teat_set =  torchvision.datasets.CIFAR10(...)</span><br></pre></td></tr></table></figure></li></ul><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><ul><li><p>二分类问题：只有两个类，非0即1。通常计算$P(\hat{y}&#x3D;1)$</p></li><li><p>逻辑函数（Sigmoid）：将函数值由实数空间映射到 [0, 1] 内。具有性质：（1）值域[0, 1] （2）单增（3）导数为饱和函数</p></li><li><p>Logistic Function（最常用的Sigmoid函数）:  $\sigma(x)&#x3D;\frac{1}{1+e^{-x}}$</p></li><li><p>Logistics回归模型：$\hat y &#x3D; \sigma(x*w+b)$</p></li><li><p>损失函数（BCE loss）：交叉熵原理</p></li></ul><p>二分类原理损失函数：$loss &#x3D; -(y\log \hat y +(1-y)\log (1-\hat y))$</p><p>小批量二分类损失函数：$loss &#x3D; -\frac{1}{N} \Sigma_{n&#x3D;1}^{N} y_n \log \hat{y_n}+(1-\hat{y_n} \log(1-\hat{y_n}))$</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># Prepare dataset</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])</span><br><span class="line"><span class="comment"># Design model using Class</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegressionModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LogisticRegressionModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y_pred = torch.sigmoid(self.linear(x))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">model = LogisticRegressionModel()</span><br><span class="line"><span class="comment"># Construct loss and optimizer</span></span><br><span class="line">criterion = torch.nn.BCELoss(size_average=<span class="literal">False</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># Training cycle</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">200</span>)</span><br><span class="line">x_t = torch.Tensor(x).view((<span class="number">200</span>, <span class="number">1</span>))</span><br><span class="line">y_t = model(x_t)</span><br><span class="line">y = y_t.data.numpy()</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">10</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>], c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Hours&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Probability of Pass&#x27;</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 刘二大人学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>04-Linear-Regression</title>
      <link href="/2022/09/12/04-Linear-Regression/"/>
      <url>/2022/09/12/04-Linear-Regression/</url>
      
        <content type="html"><![CDATA[<p>学习目标：用pytorch提供的工具实现线性回归（linear regression）、构造随机梯度下降优化器sgd。</p><h2 id="PyTorch写神经网络的步骤"><a href="#PyTorch写神经网络的步骤" class="headerlink" title="PyTorch写神经网络的步骤"></a>PyTorch写神经网络的步骤</h2><ul><li><ol><li>准备数据集：后面会说构建工具</li></ol></li><li><ol start="2"><li>设计模型：计算$\hat{y}$</li></ol></li><li><ol start="3"><li>计算损失函数和优化器：需要使用pytorch中的API</li></ol></li><li><ol start="4"><li>构建训练周期：前馈、反馈、更新…</li></ol></li></ul><h2 id="课堂实例"><a href="#课堂实例" class="headerlink" title="课堂实例"></a>课堂实例</h2><p>将模型定义成类，后续可以逐渐扩展</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment">#准备数据</span></span><br><span class="line">X_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>], [<span class="number">4.0</span>], [<span class="number">6.0</span>]])</span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(torch.nn.Module):<span class="comment">#模型继承自Moduel</span></span><br><span class="line"><span class="comment"># 必须实现两个函数 __init__ 和 forward(self, x),必须叫这个名字</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):<span class="comment">#构造函数，初始化对象时默认调用的函数</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__<span class="comment">#调用父类的构造函数</span></span><br><span class="line">        self.lnear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)<span class="comment">#构造一个线性对象，包括权重和偏置，</span></span><br><span class="line">        <span class="comment"># class torch.nn.Linear(in_features, out_features, bias=True), 前两个参数为输入输出的维度，最后为是否需要计算偏置，默认是True</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forwad</span>(<span class="params">self, x</span>):<span class="comment"># 在前馈过程中所要执行的计算，Moduel中会自动完成backward的计算，自己构造计算图</span></span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">model = LinearModel()<span class="comment">#实例化，可直接被调用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建损失函数和优化器</span></span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># class torch.nn.MSELoss(size_average=True, reduce=True)  第一个参数表示是否要求均值，第二个参数表示是否要降维</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameter(), lr=<span class="number">0.01</span>)<span class="comment">#优化器不会构建计算图，lr表示学习率，torch支持在不同地方使用不同学习率</span></span><br><span class="line"><span class="comment"># class torch.optim.SGD(params, lr=&lt;&gt;, momentum=0, dampening=0, weight_decay=0, nesterov=False)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_pred = model(x_data)<span class="comment">#forwad: predict，计算y_pred</span></span><br><span class="line">    loss = criterion(y_pred, y_data)<span class="comment">#计算损失</span></span><br><span class="line">    <span class="built_in">print</span>(epoch, loss)<span class="comment">#loss会自动调用__str__()函数，不会产生计算图</span></span><br><span class="line">    optimizer.zero_grad()<span class="comment">#所有梯度归零</span></span><br><span class="line">    loss.backward()<span class="comment">#反向传播</span></span><br><span class="line">    optimizer.step()<span class="comment">#更新</span></span><br><span class="line"><span class="comment"># 打印权重和偏置</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w=&#x27;</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b=&#x27;</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line"><span class="comment">#测试模型</span></span><br><span class="line">x_test = torch.Tensor([<span class="number">4.0</span>])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y_pred=&#x27;</span>, y_test.data)</span><br></pre></td></tr></table></figure><h2 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h2><ul><li><p>使用不同的优化器，观察损失</p><ul><li>torch.optim.Adagrad</li><li>torch.optim.Adam</li><li>torch.optim.Adamax</li><li>torch.optim.ASGD</li><li>torch.optim.LBFGS</li><li>torch.optim.RMSprop</li><li>torch.optim.Rprop</li><li>torch.optim.SGD</li></ul></li><li><p>代码实现</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Linear Regression with PyTorch</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#Prepare datatest</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>], [<span class="number">4.0</span>], [<span class="number">6.0</span>]])</span><br><span class="line">epoch_list = []</span><br><span class="line">loss_list = []</span><br><span class="line"><span class="comment"># Design model using Class</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span> (torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">model = LinearModel()</span><br><span class="line"><span class="comment"># Construct Loss and Optimizer</span></span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># Training cycle</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    loss_list.append(loss.item())</span><br><span class="line">    epoch_list.append(epoch)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w=&#x27;</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b=&#x27;</span>, model.linear.bias.item())</span><br><span class="line"><span class="comment"># Test the model</span></span><br><span class="line">x_test = torch.Tensor([<span class="number">4.0</span>])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y_pred&#x27;</span>, y_test.data)</span><br><span class="line"></span><br><span class="line">plt.plot(epoch_list, loss_list)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li><li><p>更多pytorch使用可以参考官网：<a href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html">https://pytorch.org/tutorials/beginner/pytorch_with_examples.html</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 刘二大人学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>03-Back-Propagation</title>
      <link href="/2022/09/12/03-Back-Propagation/"/>
      <url>/2022/09/12/03-Back-Propagation/</url>
      
        <content type="html"><![CDATA[<h3 id="反向传播算法BP"><a href="#反向传播算法BP" class="headerlink" title="反向传播算法BP"></a>反向传播算法BP</h3><ul><li>对于简单线性模型$y &#x3D; x* \omega$，将其看作简单神经网络。神经网络的训练过程即为权重$\omega \ $的更新，更新$\omega$需要计算损失函数loss的偏导数，参考梯度下降法。</li><li>对于简单模型，存在解析式，存在计算公式。对于下图中的复杂计算模型，存在多个权重，解析式复杂。</li><li>提出：将复杂网络看作计算图，构建一种能够在图中传播梯度的算法，再根据链式法则求出梯度——BP。</li><li>线性模型可以进行化简，多层也可化简为单层。为了增加模型复杂度，可对每层的输出增加一个非线性变换函数。</li></ul><p><img src="/assets/image-20220912143405809.png" alt="image-20220912143405809"></p><h3 id="反向传播过程"><a href="#反向传播过程" class="headerlink" title="反向传播过程"></a>反向传播过程</h3><ul><li>前馈(Forward)：从神经元某处x（输入或者中间层）经过函数$f(x,\omega)$计算输出值z，再计算损失值loss的过程，在此过程中，计算模块回计算并储存$\frac{\partial z}{\partial x} $和$\frac{\partial z}{\partial \omega} $的值。</li><li>反向(Backward)：求出loss后，该层神经元会将$\frac{\partial L}{\partial z}$的值反向传播给上一层神经元，再与前馈计算中所储存的偏导数相乘，即可得到$\frac{\partial L}{\partial \omega}$，可以用于$\omega$的更新。</li></ul><h3 id="Pytorch中的计算"><a href="#Pytorch中的计算" class="headerlink" title="Pytorch中的计算"></a>Pytorch中的计算</h3><ul><li><p>Tensor：是构建动态计算图的重要组成部分，用于存放数据，可以存放标量、向量、矩阵等。本质是一个类，包括data($\omega$)和grad($\frac{\partial Loss}{\partial \omega}$)两个成员。</p></li><li><p>使用方法：在进行运算时，会将计算对象自动转换成Tensor类型，注意类型。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">imort torch</span><br><span class="line">w = torch.Tensor([1.0])方括号里为需要储存的数据</span><br><span class="line">w.requires_grad = True  默认不计算梯度，要计算时需要指定打开</span><br><span class="line"></span><br><span class="line">l.backward()释放计算图，删除计算图中的数据</span><br><span class="line">w.grad.item()取出data变成一个数值，w.grad也是一个Tensor对象</span><br><span class="line"></span><br><span class="line">w.grad.data.zero()将梯度数据清零，防止计算L2时累加</span><br></pre></td></tr></table></figure><p><em>每调用一次Loss函数都是在构建一个计算图</em></p></li></ul><h3 id="课堂练习"><a href="#课堂练习" class="headerlink" title="课堂练习"></a>课堂练习</h3><ul><li><p>问题描述</p><p>同线性模型中的数据，使用BP算法对x&#x3D;4的结果进行预测</p></li><li><p>代码实现</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line">w = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line">w.requires_grad = <span class="literal">True</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict (before training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        <span class="comment"># forward：计算loss</span></span><br><span class="line">        l = loss(x, y)</span><br><span class="line">        l.backward()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\tgrad&#x27;</span>, x, y, w.grad.item())</span><br><span class="line">        <span class="comment"># backward：更新w</span></span><br><span class="line">        w.data = w.data - alpha * w.grad.data</span><br><span class="line">        w.grad.data.zero_()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Progress&quot;</span>, epoch, l.item())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict (after training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br></pre></td></tr></table></figure></li></ul><h3 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h3><ul><li>问题描述</li></ul><p><img src="/assets/image-20220912172557849.png" alt="image-20220912172557849"></p><ul><li><p>代码实现</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">4.0</span>, <span class="number">9.0</span>, <span class="number">16.0</span>]</span><br><span class="line"></span><br><span class="line">w1 = torch.Tensor([<span class="number">2.0</span>])</span><br><span class="line">w1.requires_grad = <span class="literal">True</span></span><br><span class="line">w2 = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line">w2.requires_grad = <span class="literal">True</span></span><br><span class="line">b = torch.Tensor([<span class="number">2.0</span>])</span><br><span class="line">b.requires_grad = <span class="literal">True</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> w1 * x**<span class="number">2</span> + w2 * x + b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict (before training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        <span class="comment"># forward：计算loss</span></span><br><span class="line">        l = loss(x, y)</span><br><span class="line">        l.backward()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\tgrad&#x27;</span>, x, y, w1.grad.item(), w2.grad.item(), b.grad.item())</span><br><span class="line">        <span class="comment"># backward：更新w</span></span><br><span class="line">        w1.data = w1.data - alpha * w1.grad.data</span><br><span class="line">        w2.data = w2.data - alpha * w2.grad.data</span><br><span class="line">        b.data = b.data - alpha * b.grad.data</span><br><span class="line"></span><br><span class="line">        w1.grad.data.zero_()</span><br><span class="line">        w2.grad.data.zero_()</span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Progress&quot;</span>, epoch, l.item())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict (after training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br></pre></td></tr></table></figure></li></ul><p><em>矩阵计算可参考书籍《The Matrix Cook》</em></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 刘二大人学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>02-Gradient-Desent</title>
      <link href="/2022/09/11/02-Gradient-Desent/"/>
      <url>/2022/09/11/02-Gradient-Desent/</url>
      
        <content type="html"><![CDATA[<h3 id="问题引出"><a href="#问题引出" class="headerlink" title="问题引出"></a>问题引出</h3><ul><li>穷举法：当穷举区间过大时，效率低下</li><li>分治法：当损失函数是非凸函数时，容易取到局部最小值，当区间过大时，效率低下</li></ul><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><ul><li>梯度：$\frac{\partial{cost}}{\partial{\omega}}$，学习率：$\alpha$，更新：$\omega &#x3D; \omega - \alpha\frac{\partial{cost}}{\partial{\omega}}$</li><li>特点：属于贪心算法，对于非凸函数，容易取到局部最优。深度学习神经网络中，局部最优点较少，但是存在<strong>鞍点</strong>。</li><li>$\frac{\partial{cost(\omega)}}{\partial{\omega}} &#x3D; \frac{1}{N}\Sigma_{n&#x3D;1}^N2\cdot x_n \cdot (x_n \cdot \omega - y_n)$，其中 $\omega &#x3D; \omega - \alpha \frac{1}{N}\Sigma_{n&#x3D;1}^N2\cdot x_n \cdot (x_n \cdot \omega - y_n)$</li><li>学习率取太大，容易训练发散</li></ul><h3 id="课堂练习"><a href="#课堂练习" class="headerlink" title="课堂练习"></a>课堂练习</h3><ul><li><p>问题描述</p><p>同样构建数据集，采用简单线性模型进行预测，用梯度下降算法求$\omega$ 值。</p></li><li><p>代码及结果</p></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cost</span>(<span class="params">xs, ys</span>):</span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):</span><br><span class="line">        y_pred = forward(x)</span><br><span class="line">        cost += (y_pred - y) ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> cost / <span class="built_in">len</span>(xs)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">xs, ys</span>):</span><br><span class="line">    grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):</span><br><span class="line">        grad += <span class="number">2</span> * x * (forward(x) - y)</span><br><span class="line">    <span class="keyword">return</span> grad / <span class="built_in">len</span>(xs)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;Predict (before training&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line">epoch_list = []</span><br><span class="line">cost_list = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    cost_val = cost(x_data, y_data)</span><br><span class="line">    grad_val = gradient(x_data, y_data)</span><br><span class="line">    w -= alpha * grad_val</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, epoch, <span class="string">&#x27;w=&#x27;</span>, w, <span class="string">&#x27;loss=&#x27;</span>, cost_val)</span><br><span class="line">    epoch_list.append(epoch)</span><br><span class="line">    cost_list.append(cost_val)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">plt.plot(epoch_list, cost_list)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="随机梯度下降-SGD"><a href="#随机梯度下降-SGD" class="headerlink" title="随机梯度下降-SGD"></a>随机梯度下降-SGD</h3><ul><li><p>Stochastic Gradient Descent: $\omega &#x3D; \omega - \alpha\frac{\partial{loss}}{\partial{\omega}}$,   $\frac{\partial{loss_n}}{\partial{\omega}} &#x3D; 2 \cdot x_n \cdot (x_n \cdot \omega -y_n)$</p></li><li><p>优点：有可能跨越鞍点，在神经网络中常用</p></li><li><p>存在问题：$\omega$ 的更新依赖于每一个训练集，就是下一个$\omega$ 的值取决于上一次计算，所以不能并行计算。</p></li><li><p>采取折衷的办法：Mini-Batch，批量随机梯度下降，取若干个训练集为一组进行随机梯度下降计算。</p></li><li><p>代码实现：</p></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> (x * w - y)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x * (w * x - y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;Predict (before training&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        grad = gradient(x, y)</span><br><span class="line">        w -= alpha * grad</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\tgrad: &#x27;</span>, x, y, grad)</span><br><span class="line">        l = loss(x, y)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;progress: &quot;</span>, epoch, <span class="string">&#x27;w=&#x27;</span>, w, <span class="string">&quot;loss=&quot;</span>, l)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 刘二大人学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01-Linear-Model</title>
      <link href="/2022/09/08/01-Linear-model/"/>
      <url>/2022/09/08/01-Linear-model/</url>
      
        <content type="html"><![CDATA[<h2 id="深度学习准备过程"><a href="#深度学习准备过程" class="headerlink" title="深度学习准备过程"></a>深度学习准备过程</h2><ul><li>准备数据集</li><li>选择模型</li><li>模型训练：a. 有监督训练；b. 无监督训练。</li><li>推理预测</li></ul><h2 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h2><p>$$<br>\hat{y} &#x3D; x*\omega + b \ ;\  \hat{y} &#x3D; x * \omega<br>\tag{1}<br>$$</p><p>其中，$\omega$和<em>b</em>是模型参数，训练过程就是确定模型参数的过程。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>训练误差：Training Loss (Error)<br>$$<br>\rm loss &#x3D; (\hat{y}-y)^2&#x3D;(x*{\omega} - y)^2<br>\tag{2}<br>$$<br>均方差：Mean Square Error<br>$$<br>\rm cost &#x3D; \frac{1}{N}\sum_1^N(\hat{y}_n-y_n)^2<br>\tag{3}<br>$$</p><h2 id="课堂练习"><a href="#课堂练习" class="headerlink" title="课堂练习"></a>课堂练习</h2><ul><li><p>问题描述</p><p>学生花费时间与其所获得分数的关系，使用$y &#x3D; x * \omega$模型</p></li></ul><table><thead><tr><th>x(hours)</th><th>y(points)</th></tr></thead><tbody><tr><td>1</td><td>2</td></tr><tr><td>2</td><td>4</td></tr><tr><td>3</td><td>6</td></tr><tr><td>4</td><td>?</td></tr></tbody></table><ul><li><p>代码及结果展示</p><p>项目名称：Linear_model01</p></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"><span class="comment"># 前馈计算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x,y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"><span class="comment"># 创建空列表</span></span><br><span class="line">w_list = []</span><br><span class="line">mse_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w&#x27;</span>, w)</span><br><span class="line">    l_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        y_pred_val = forward(x_val)</span><br><span class="line">        loss_val = loss(x_val, y_val)</span><br><span class="line">        l_sum += loss_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>, x_val, y_val, y_pred_val, loss_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;MSE=&#x27;</span>, l_sum / <span class="number">3</span>)</span><br><span class="line">    w_list.append(w)</span><br><span class="line">    mse_list.append(l_sum / <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 使用matplotlib画图</span></span><br><span class="line">plt.plot(w_list, mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><em>Up主提出：可以利用Visdom创建web服务，进行训练情况的可视化，还可以考虑断点重开的问题。</em></p><h2 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h2><ul><li><p>问题描述</p><p>同上述问题，使用$y &#x3D; x * \omega + b$模型。</p></li><li><p>代码及结果展示</p></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建 y = 2*x + 1 的数据集</span></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">3.0</span>, <span class="number">5.0</span>, <span class="number">7.0</span>]</span><br><span class="line">W, B = np.arange(<span class="number">0.0</span>, <span class="number">4.0</span>, <span class="number">0.1</span>), np.arange(<span class="number">0.0</span>, <span class="number">4.0</span>, <span class="number">0.1</span>)</span><br><span class="line">w, b = np.meshgrid(W, B)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前馈计算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x,y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line">l_sum = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">    y_pred_val = forward(x_val)</span><br><span class="line">    loss_val = loss(x_val, y_val)</span><br><span class="line">    l_sum += loss_val</span><br><span class="line"></span><br><span class="line"><span class="comment"># 引入matplotlib 3D画图</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.plot_surface(w, b, l_sum/<span class="number">3</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&quot;w&quot;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&quot;b&quot;</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">&quot;loss&quot;</span>)</span><br><span class="line">ax.text(<span class="number">0.2</span>, <span class="number">2</span>, <span class="number">43</span>, <span class="string">&quot;Cost Value&quot;</span>, color=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><p>画3d曲线和曲面图可参考如下链接</p><p>[1] <a href="https://matplotlib.org/stable/tutorials/toolkits/mplot3d.html">https://matplotlib.org/stable/tutorials/toolkits/mplot3d.html</a></p><p>[2] <a href="https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html#numpy.meshgrid">https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html#numpy.meshgrid</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 刘二大人学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用科研网站合集</title>
      <link href="/2022/09/07/%E5%B8%B8%E7%94%A8%E7%A7%91%E7%A0%94%E7%BD%91%E7%AB%99%E5%90%88%E9%9B%86/"/>
      <url>/2022/09/07/%E5%B8%B8%E7%94%A8%E7%A7%91%E7%A0%94%E7%BD%91%E7%AB%99%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<p>本文档收录了在学习、生活和科研工作中，常用到的一些网站。包括了：文献查询、论文写作、CFD学习、编程入门、看球追剧等等 <em>^_^</em>。善用网站资源能够提高工作效率！推荐搜索引擎：谷歌，bing。</p><h2 id="1-文献查询网站"><a href="#1-文献查询网站" class="headerlink" title="1. 文献查询网站"></a>1. 文献查询网站</h2><ul><li><p>知网：<a href="https://xueshu.baidu.com/">https://xueshu.baidu.com/</a>  </p></li><li><p>百度学术：<a href="https://xueshu.baidu.com/">https://xueshu.baidu.com/</a> </p></li><li><p>文献搜索： <a href="http://459.org/">http://459.org/</a> </p></li><li><p>国外图书、学位论文：<a href="http://cdsweb.cern.ch/">http://cdsweb.cern.ch/</a></p></li><li><p>智能学术引擎：<a href="https://www.semanticscholar.org/">https://www.semanticscholar.org/</a>  —–新SCI-hub</p></li><li><p>Web of Science: <a href="http://apps.webofknowledge.com/">http://apps.webofknowledge.com</a></p></li><li><p>超新图书馆： <a href="http://www.sslibrary.com/">http://www.sslibrary.com/</a>    —–中文图书</p></li></ul><h2 id="2-论文写作网站"><a href="#2-论文写作网站" class="headerlink" title="2. 论文写作网站"></a>2. 论文写作网站</h2><ul><li>常用写作句式：<a href="https://www.phrasebank.manchester.ac.uk/">https://www.phrasebank.manchester.ac.uk/</a>    —-曼切斯特大学</li><li>同、反义词：<a href="https://www.wordhippo.com/">https://www.wordhippo.com/</a></li><li>句子润色： <a href="https://rewordify.com/">https://rewordify.com/</a> </li><li>51听力： <a href="https://www.51voa.com/">https://www.51voa.com/</a> </li><li>Overleaf写作：<a href="https://www.overleaf.com/">https://www.overleaf.com/</a></li></ul><h2 id="3-CFD学习网站"><a href="#3-CFD学习网站" class="headerlink" title="3. CFD学习网站"></a>3. CFD学习网站</h2><ul><li>东岳流体： <a href="http://www.dyfluid.com/">http://www.dyfluid.com/</a> </li><li>OpenFOAM官网： <a href="https://openfoam.org/">https://openfoam.org/</a></li><li>wolfDynamics暑假OpenFOAM培训课程：<a href="http://www.wolfdynamics.com/tutorials.html?layout=edit&amp;id=163">http://www.wolfdynamics.com/tutorials.html?layout=edit&amp;id=163</a> </li><li>Ubunut安装：<a href="https://www.jishulink.com/content/post/363858">https://www.jishulink.com/content/post/363858</a> </li><li>OpenFOAM学习资料汇总： <a href="https://openfoam.top/studyMaterials/?tdsourcetag=s_pcqq_aiomsg#official-websites">https://openfoam.top/studyMaterials/?tdsourcetag=s_pcqq_aiomsg#official-websites</a></li><li>湍流系数计算：<a href="https://blog.csdn.net/weixin_42465030/article/details/112286955">https://blog.csdn.net/weixin_42465030/article/details/112286955</a> </li><li>高算作业提交：<a href="https://www.cnblogs.com/dahu-daqing/p/12693334.html">https://www.cnblogs.com/dahu-daqing/p/12693334.html</a></li></ul><h2 id="4-编程学习网站"><a href="#4-编程学习网站" class="headerlink" title="4. 编程学习网站"></a>4. 编程学习网站</h2><ul><li>CS自学指南：<a href="https://csdiy.wiki/">https://csdiy.wiki/</a></li><li>python画图：<a href="https://matplotlib.org/">https://matplotlib.org/</a></li><li>菜鸟编程：<a href="https://www.runoob.com/cplusplus/cpp-data-structures.html">https://www.runoob.com/cplusplus/cpp-data-structures.html</a> </li><li>常用Linux命令：<a href="http://linuxcommand.org/index.php">http://linuxcommand.org/index.php</a></li></ul><h2 id="5-科研实用网站"><a href="#5-科研实用网站" class="headerlink" title="5. 科研实用网站"></a>5. 科研实用网站</h2><ul><li>优秀网站合集：<a href="https://www.tboxn.com/">https://www.tboxn.com/</a></li><li>动画制作：<a href="https://www.gimp.org/">https://www.gimp.org/</a></li><li>图片压缩：<a href="https://zh.geekersoft.com/image-compress-online.html">https://zh.geekersoft.com/image-compress-online.html</a> </li><li>图文转换：<a href="https://saas.xfyun.cn/ocr?ch=socrod">https://saas.xfyun.cn/ocr?ch=socrod</a> </li><li>抠图网站：<a href="http://www.remove.bg/">www.remove.bg</a></li><li>Latex公式在线编辑：<a href="https://latexlive.com/">https://latexlive.com/</a> </li><li>Latex公式语法：<a href="https://www.cnblogs.com/houkai/p/3399646.html">https://www.cnblogs.com/houkai/p/3399646.html</a></li><li>markdown语法：<a href="https://www.runoob.com/markdown/md-tutorial.html">https://www.runoob.com/markdown/md-tutorial.html</a></li></ul><h2 id="6-看球追剧网站"><a href="#6-看球追剧网站" class="headerlink" title="6. 看球追剧网站"></a>6. 看球追剧网站</h2><ul><li>美剧1：<a href="https://www.libvio.me/">https://www.libvio.me/</a></li><li>美剧2：<a href="https://www.mjw2020.com/">https://www.mjw2020.com/</a> </li><li>优直播：<a href="https://www.yoozhibo.com/">https://www.yoozhibo.com/</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 网站合集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 常用网站 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CFD参考文档</title>
      <link href="/2022/09/07/CFD%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3/"/>
      <url>/2022/09/07/CFD%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3/</url>
      
        <content type="html"><![CDATA[<h3 id="ANSYS"><a href="#ANSYS" class="headerlink" title="ANSYS"></a>ANSYS</h3><ul><li><p>常见湍流模型介绍：<a href="https://mp.weixin.qq.com/s/BZG77Wh7zo4yDrbgOKK0Ig">https://mp.weixin.qq.com/s/BZG77Wh7zo4yDrbgOKK0Ig</a></p></li><li><p>CFD之道2020年汇总：<a href="https://mp.weixin.qq.com/s/62uocx-q1kYCLKWcWjYObg">https://mp.weixin.qq.com/s/62uocx-q1kYCLKWcWjYObg</a></p></li><li><p>循环通道计算(热管)：[1] <a href="https://mp.weixin.qq.com/s/npxEOJTlM-iHhuGGEyaejg">https://mp.weixin.qq.com/s/npxEOJTlM-iHhuGGEyaejg</a>  [2] <a href="https://mp.weixin.qq.com/s/4F4WfVBArxRodJstLQjTxw">https://mp.weixin.qq.com/s/4F4WfVBArxRodJstLQjTxw</a></p></li><li><p>矩阵与张量：<a href="https://mp.weixin.qq.com/s/2Pas3-WF_waQqn_D8wn4Gw">https://mp.weixin.qq.com/s/2Pas3-WF_waQqn_D8wn4Gw</a></p></li><li><p>表达式功能：<a href="https://mp.weixin.qq.com/s/c_DzTLFrjeSOKVtlqCQ18w">https://mp.weixin.qq.com/s/c_DzTLFrjeSOKVtlqCQ18w</a></p></li></ul><h3 id="Julia-CFD"><a href="#Julia-CFD" class="headerlink" title="Julia CFD"></a>Julia CFD</h3><ul><li>CFD之道系列：<a href="https://mp.weixin.qq.com/s/8aeqJ1KNrUrMZHwGZYSX9w">https://mp.weixin.qq.com/s/8aeqJ1KNrUrMZHwGZYSX9w</a></li></ul><h3 id="C-技术博客"><a href="#C-技术博客" class="headerlink" title="C++技术博客"></a>C++技术博客</h3><ul><li>作者：等疾风：<a href="https://codesire-deng.github.io/">https://codesire-deng.github.io/</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 网站合集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CFD计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>example</title>
      <link href="/2022/09/07/example/"/>
      <url>/2022/09/07/example/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/09/04/hello-world/"/>
      <url>/2022/09/04/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> 主题配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
